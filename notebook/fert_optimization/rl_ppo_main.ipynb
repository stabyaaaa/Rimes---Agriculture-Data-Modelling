{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a533c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gym import Env, spaces\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/final/merged_dataset_final_all_pp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df67ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='palika_num', inplace=True)\n",
    "df.drop(columns='district', inplace=True)\n",
    "df.drop(columns='province', inplace=True)\n",
    "df.drop(columns='palika', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0132d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # Load your data (replace with actual df)\n",
    "# # ---------------------------\n",
    "# # df = pd.read_csv('your_file.csv')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # Drop problematic columns\n",
    "\n",
    "# # Define state (soil features) and action (fertilizers)\n",
    "# soil_cols = ['ph','organic_matter','total_nitrogen','potassium','p2o5','boron','zinc',\n",
    "#              'sand','clay','slit','parentsoil','crop','variety']\n",
    "# fert_cols = ['UREA1','UREA2','UREA3','DAP','MOP','organic','boron_fert']\n",
    "\n",
    "# # Fill missing values\n",
    "# for col in soil_cols:\n",
    "#     df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# for col in fert_cols:\n",
    "#     df[col].fillna(0, inplace=True)\n",
    "# X = df[soil_cols]\n",
    "# y = df[fert_cols]\n",
    "# # Take only 5000 samples\n",
    "# df_sample = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[soil_cols] = scaler.fit_transform(X_train[soil_cols])  # fit on train only\n",
    "# X_test[soil_cols] = scaler.transform(X_test[soil_cols]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e6b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Environment\n",
    "# # ---------------------------\n",
    "# class FertilizerEnv:\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data.reset_index(drop=True)\n",
    "#         self.n_steps = len(data)\n",
    "#         self.n_features = data.shape[1]\n",
    "#         self.max_fertilizer = np.array([100, 100, 100, 50, 50, 5, 5], dtype=np.float32)\n",
    "#         self.current_idx = 0\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.current_idx = 0\n",
    "#         return self.data.iloc[self.current_idx].values.astype(np.float32)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         action = np.clip(action, 0, self.max_fertilizer)\n",
    "#         reward = -np.sum(np.square(action / self.max_fertilizer))  # closer to 0 is better\n",
    "#         self.current_idx += 1\n",
    "#         done = self.current_idx >= self.n_steps\n",
    "#         obs = self.data.iloc[self.current_idx % self.n_steps].values.astype(np.float32) if not done else np.zeros(self.n_features)\n",
    "#         return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f7ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO Agent\n",
    "# # ---------------------------\n",
    "# class PPOAgent:\n",
    "#     def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, clip=0.2, epochs=10):\n",
    "#         self.policy = PolicyNetwork(input_dim, action_dim)\n",
    "#         self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "#         self.gamma = gamma\n",
    "#         self.clip = clip\n",
    "#         self.epochs = epochs\n",
    "#         self.memory = []\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         state = torch.FloatTensor(state).unsqueeze(0)\n",
    "#         mu, std = self.policy(state)\n",
    "#         dist = Normal(mu, std)\n",
    "#         action = dist.sample()\n",
    "#         log_prob = dist.log_prob(action).sum(dim=1)\n",
    "#         return action.detach().numpy()[0], log_prob.detach()\n",
    "\n",
    "#     def store(self, state, action, log_prob, reward, done):\n",
    "#         self.memory.append((state, action, log_prob, reward, done))\n",
    "\n",
    "#     def compute_returns(self, rewards, dones):\n",
    "#         returns = []\n",
    "#         R = 0\n",
    "#         for r, done in zip(reversed(rewards), reversed(dones)):\n",
    "#             if done:\n",
    "#                 R = 0\n",
    "#             R = r + self.gamma * R\n",
    "#             returns.insert(0, R)\n",
    "#         return returns\n",
    "\n",
    "#     def train(self):\n",
    "#         if len(self.memory) == 0:\n",
    "#             return\n",
    "#         states, actions, old_log_probs, rewards, dones = zip(*self.memory)\n",
    "#         states = torch.FloatTensor(states)\n",
    "#         actions = torch.FloatTensor(actions)\n",
    "#         old_log_probs = torch.stack(old_log_probs)\n",
    "#         returns = torch.FloatTensor(self.compute_returns(rewards, dones))\n",
    "\n",
    "#         for _ in range(self.epochs):\n",
    "#             mu, std = self.policy(states)\n",
    "#             dist = Normal(mu, std)\n",
    "#             log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "#             ratios = torch.exp(log_probs - old_log_probs)\n",
    "#             advantages = returns - returns.mean()\n",
    "#             surr1 = ratios * advantages\n",
    "#             surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * advantages\n",
    "#             loss = -torch.min(surr1, surr2).mean()\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "#         self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c66a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO Network\n",
    "# # ---------------------------\n",
    "# class PolicyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, action_dim):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.mu = nn.Linear(128, action_dim)\n",
    "#         self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.net(x)\n",
    "#         mu = self.mu(x)\n",
    "#         std = torch.exp(self.log_std)\n",
    "#         return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbd1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO Network\n",
    "# # ---------------------------\n",
    "# class PolicyNetwork(nn.Module):\n",
    "#     def __init__(self, input_dim, action_dim):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.mu = nn.Linear(128, action_dim)\n",
    "#         self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.net(x)\n",
    "#         mu = self.mu(x)\n",
    "#         std = torch.exp(self.log_std)\n",
    "#         return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ce37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PPO Agent\n",
    "# # ---------------------------\n",
    "# class PPOAgent:\n",
    "#     def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, clip=0.2, epochs=10):\n",
    "#         self.policy = PolicyNetwork(input_dim, action_dim)\n",
    "#         self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "#         self.gamma = gamma\n",
    "#         self.clip = clip\n",
    "#         self.epochs = epochs\n",
    "#         self.memory = []\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         state = torch.FloatTensor(state).unsqueeze(0)\n",
    "#         mu, std = self.policy(state)\n",
    "#         dist = Normal(mu, std)\n",
    "#         action = dist.sample()\n",
    "#         log_prob = dist.log_prob(action).sum(dim=1)\n",
    "#         return action.detach().numpy()[0], log_prob.detach()\n",
    "\n",
    "#     def store(self, state, action, log_prob, reward, done):\n",
    "#         self.memory.append((state, action, log_prob, reward, done))\n",
    "\n",
    "#     def compute_returns(self, rewards, dones):\n",
    "#         returns = []\n",
    "#         R = 0\n",
    "#         for r, done in zip(reversed(rewards), reversed(dones)):\n",
    "#             if done:\n",
    "#                 R = 0\n",
    "#             R = r + self.gamma * R\n",
    "#             returns.insert(0, R)\n",
    "#         return returns\n",
    "\n",
    "#     def train(self):\n",
    "#         if len(self.memory) == 0:\n",
    "#             return\n",
    "#         states, actions, old_log_probs, rewards, dones = zip(*self.memory)\n",
    "#         states = torch.FloatTensor(states)\n",
    "#         actions = torch.FloatTensor(actions)\n",
    "#         old_log_probs = torch.stack(old_log_probs)\n",
    "#         returns = torch.FloatTensor(self.compute_returns(rewards, dones))\n",
    "\n",
    "#         for _ in range(self.epochs):\n",
    "#             mu, std = self.policy(states)\n",
    "#             dist = Normal(mu, std)\n",
    "#             log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "#             ratios = torch.exp(log_probs - old_log_probs)\n",
    "#             advantages = returns - returns.mean()\n",
    "#             surr1 = ratios * advantages\n",
    "#             surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * advantages\n",
    "#             loss = -torch.min(surr1, surr2).mean()\n",
    "#             self.optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             self.optimizer.step()\n",
    "#         self.memory = []\n",
    "\n",
    "# # ---------------------------\n",
    "# # Split features for environment\n",
    "# # ---------------------------\n",
    "# X = df_sample[soil_cols]\n",
    "# y = df_sample[fert_cols]  # not used in this reward setup\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# train_env_data = X_train.reset_index(drop=True)\n",
    "# test_env_data = X_test.reset_index(drop=True)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Initialize PPO agent and environment\n",
    "# # ---------------------------\n",
    "# input_dim = X_train.shape[1]\n",
    "# action_dim = len(fert_cols)\n",
    "\n",
    "# env = FertilizerEnv(train_env_data)\n",
    "# agent = PPOAgent(input_dim=input_dim, action_dim=action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9028bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # PPO Training Loop\n",
    "# # ---------------------------\n",
    "# n_episodes = 200\n",
    "# for ep in range(n_episodes):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         action, log_prob = agent.select_action(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         agent.store(state, action, log_prob, reward, done)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "\n",
    "#     agent.train()\n",
    "#     print(f\"Episode {ep+1}/{n_episodes}, Total Reward: {total_reward:.4f}\")\n",
    "\n",
    "# # ---------------------------\n",
    "# # Test PPO agent\n",
    "# # ---------------------------\n",
    "# env_test = FertilizerEnv(test_env_data)\n",
    "# state = env_test.reset()\n",
    "# done = False\n",
    "# predictions = []\n",
    "\n",
    "# while not done:\n",
    "#     action, _ = agent.select_action(state)\n",
    "#     predictions.append(action)\n",
    "#     state, _, done, _ = env_test.step(action)\n",
    "\n",
    "# predictions = np.array(predictions)\n",
    "# print(\"Sample fertilizer predictions:\\n\", predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb4e68",
   "metadata": {},
   "source": [
    "Good One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b165fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 0.8596, Val Loss: 0.8092\n",
      "Epoch 20/200, Train Loss: 0.1790, Val Loss: 0.1736\n",
      "Epoch 40/200, Train Loss: 0.0418, Val Loss: 0.0434\n",
      "Epoch 60/200, Train Loss: 0.0153, Val Loss: 0.0165\n",
      "Epoch 80/200, Train Loss: 0.0068, Val Loss: 0.0072\n",
      "Epoch 100/200, Train Loss: 0.0033, Val Loss: 0.0034\n",
      "Epoch 120/200, Train Loss: 0.0017, Val Loss: 0.0018\n",
      "Epoch 140/200, Train Loss: 0.0009, Val Loss: 0.0010\n",
      "Epoch 160/200, Train Loss: 0.0006, Val Loss: 0.0005\n",
      "Epoch 180/200, Train Loss: 0.0003, Val Loss: 0.0003\n",
      "Epoch 200/200, Train Loss: 0.0002, Val Loss: 0.0002\n",
      "\n",
      "Final Train RMSE: 0.0075, Validation RMSE: 0.0071\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =====================================\n",
    "# 1 Load and preprocess your dataset\n",
    "# =====================================\n",
    "soil_cols = [\n",
    "    'ph','organic_matter','total_nitrogen','potassium','p2o5','boron','zinc',\n",
    "    'sand','clay','slit','parentsoil','crop','variety'\n",
    "]\n",
    "fert_cols = ['UREA1','UREA2','UREA3','DAP','MOP','organic']\n",
    "\n",
    "# Make sure df is your dataset\n",
    "df = df.dropna(subset=soil_cols + fert_cols)\n",
    "df = df.sample(min(4000, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = df[soil_cols]\n",
    "y = df[fert_cols]\n",
    "\n",
    "# Normalize features\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Normalize target fertilizers to [0,1] for stable training\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# =====================================\n",
    "# 2 Neural Network for Regression\n",
    "# =====================================\n",
    "class FertNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FertNet(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# =====================================\n",
    "# 3 Train the model\n",
    "# =====================================\n",
    "epochs = 200\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_list.append(loss.item())\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test)\n",
    "        val_loss = criterion(val_outputs, y_test)\n",
    "        val_loss_list.append(val_loss.item())\n",
    "\n",
    "    if ep % 20 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 4 Evaluate RMSE\n",
    "# =====================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_preds = model(X_train)\n",
    "    val_preds = model(X_test)\n",
    "\n",
    "# Inverse scale to original fertilizer units\n",
    "train_preds_orig = scaler_y.inverse_transform(train_preds.numpy())\n",
    "val_preds_orig = scaler_y.inverse_transform(val_preds.numpy())\n",
    "y_train_orig = scaler_y.inverse_transform(y_train.numpy())\n",
    "y_test_orig = scaler_y.inverse_transform(y_test.numpy())\n",
    "\n",
    "train_rmse = np.sqrt(np.mean((train_preds_orig - y_train_orig) ** 2))\n",
    "val_rmse = np.sqrt(np.mean((val_preds_orig - y_test_orig) ** 2))\n",
    "\n",
    "print(f\"\\nFinal Train RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 5 Plot Loss\n",
    "# =====================================\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce164281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 1.002343, Val Loss: 0.871546\n",
      "Epoch 10/200, Train Loss: 0.462297, Val Loss: 0.444390\n",
      "Epoch 20/200, Train Loss: 0.239503, Val Loss: 0.220121\n",
      "Epoch 30/200, Train Loss: 0.112262, Val Loss: 0.105318\n",
      "Epoch 40/200, Train Loss: 0.051049, Val Loss: 0.047242\n",
      "Epoch 50/200, Train Loss: 0.026784, Val Loss: 0.025696\n",
      "Epoch 60/200, Train Loss: 0.015337, Val Loss: 0.014247\n",
      "Epoch 70/200, Train Loss: 0.009210, Val Loss: 0.008380\n",
      "Epoch 80/200, Train Loss: 0.005648, Val Loss: 0.004890\n",
      "Epoch 90/200, Train Loss: 0.003663, Val Loss: 0.003015\n",
      "Epoch 100/200, Train Loss: 0.002470, Val Loss: 0.001951\n",
      "Epoch 110/200, Train Loss: 0.001726, Val Loss: 0.001309\n",
      "Epoch 120/200, Train Loss: 0.001242, Val Loss: 0.000924\n",
      "Epoch 130/200, Train Loss: 0.000908, Val Loss: 0.000669\n",
      "Epoch 140/200, Train Loss: 0.000670, Val Loss: 0.000498\n",
      "Epoch 150/200, Train Loss: 0.000504, Val Loss: 0.000372\n",
      "Epoch 160/200, Train Loss: 0.000386, Val Loss: 0.000286\n",
      "Epoch 170/200, Train Loss: 0.000299, Val Loss: 0.000226\n",
      "Epoch 180/200, Train Loss: 0.000236, Val Loss: 0.000184\n",
      "Epoch 190/200, Train Loss: 0.000189, Val Loss: 0.000153\n",
      "Epoch 200/200, Train Loss: 0.000155, Val Loss: 0.000130\n",
      "Predicted fertilizers: {'UREA1': 0.9691876173019409, 'UREA2': 1.8085219860076904, 'UREA3': 1.810304880142212, 'DAP': 2.171752452850342, 'MOP': 1.6693230867385864}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 1 Prepare dataset\n",
    "# =========================\n",
    "soil_cols = [\n",
    "    'ph','organic_matter','total_nitrogen','potassium','p2o5','boron','zinc',\n",
    "    'sand','clay','slit','parentsoil','crop','variety'\n",
    "]\n",
    "fert_cols = ['UREA1','UREA2','UREA3','DAP','MOP']\n",
    "\n",
    "# Example: df already loaded\n",
    "df = df.dropna(subset=soil_cols + fert_cols)\n",
    "df = df.sample(min(25000, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = df[soil_cols]\n",
    "y = df[fert_cols]\n",
    "\n",
    "# Feature & target scaling\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# =========================\n",
    "# 2 Define model\n",
    "# =========================\n",
    "class FertNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FertNet(X_train.shape[1], y_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# =========================\n",
    "# 3 Train model\n",
    "# =========================\n",
    "epochs = 200\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional validation\n",
    "    if ep % 10 == 0 or ep == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test)\n",
    "            val_loss = criterion(val_outputs, y_test)\n",
    "        print(f\"Epoch {ep}/{epochs}, Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"fertilizer_model.pth\")\n",
    "\n",
    "# =========================\n",
    "# 4 Production-ready prediction\n",
    "# =========================\n",
    "# Precompute min/max for clipping\n",
    "fert_min = np.zeros(len(fert_cols))           # no negative fertilizers\n",
    "fert_max = y.max().values                     # realistic max per fertilizer\n",
    "\n",
    "def predict_fertilizer(input_features):\n",
    "    \"\"\"\n",
    "    input_features: list or array of soil features in the order of soil_cols\n",
    "    returns: array of predicted fertilizer amounts (clipped to realistic ranges)\n",
    "    \"\"\"\n",
    "    x_scaled = scaler_X.transform([input_features])\n",
    "    x_tensor = torch.tensor(x_scaled, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_scaled_pred = model(x_tensor).numpy()\n",
    "    y_pred = scaler_y.inverse_transform(y_scaled_pred)\n",
    "    # Clip to valid ranges\n",
    "    y_pred_clipped = np.clip(y_pred, fert_min, fert_max)\n",
    "    return y_pred_clipped.flatten()\n",
    "\n",
    "# =========================\n",
    "# 5 Test\n",
    "# =========================\n",
    "sample_input = X.iloc[0].values\n",
    "predicted_fert = predict_fertilizer(sample_input)\n",
    "print(\"Predicted fertilizers:\", dict(zip(fert_cols, predicted_fert)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7667c7",
   "metadata": {},
   "source": [
    "Epoch 1/200, Train Loss: 1.029936, Val Loss: 0.879038\n",
    "Epoch 10/200, Train Loss: 0.490734, Val Loss: 0.420425\n",
    "Epoch 20/200, Train Loss: 0.239749, Val Loss: 0.207890\n",
    "Epoch 30/200, Train Loss: 0.101824, Val Loss: 0.087729\n",
    "Epoch 40/200, Train Loss: 0.046947, Val Loss: 0.040441\n",
    "Epoch 50/200, Train Loss: 0.025431, Val Loss: 0.022749\n",
    "Epoch 60/200, Train Loss: 0.014405, Val Loss: 0.013354\n",
    "Epoch 70/200, Train Loss: 0.008468, Val Loss: 0.008264\n",
    "Epoch 80/200, Train Loss: 0.005191, Val Loss: 0.005307\n",
    "Epoch 90/200, Train Loss: 0.003379, Val Loss: 0.003699\n",
    "Epoch 100/200, Train Loss: 0.002331, Val Loss: 0.002728\n",
    "Epoch 110/200, Train Loss: 0.001685, Val Loss: 0.002078\n",
    "Epoch 120/200, Train Loss: 0.001259, Val Loss: 0.001639\n",
    "Epoch 130/200, Train Loss: 0.000961, Val Loss: 0.001320\n",
    "Epoch 140/200, Train Loss: 0.000755, Val Loss: 0.001093\n",
    "Epoch 150/200, Train Loss: 0.000600, Val Loss: 0.000912\n",
    "Epoch 160/200, Train Loss: 0.000489, Val Loss: 0.000775\n",
    "Epoch 170/200, Train Loss: 0.000402, Val Loss: 0.000661\n",
    "Epoch 180/200, Train Loss: 0.000333, Val Loss: 0.000564\n",
    "Epoch 190/200, Train Loss: 0.000278, Val Loss: 0.000482\n",
    "Epoch 200/200, Train Loss: 0.000232, Val Loss: 0.000412\n",
    "Predicted fertilizers: {'UREA1': 1.178892731666565, 'UREA2': 1.8103035688400269, 'UREA3': 1.811872959136963, 'DAP': 1.6326504945755005, 'MOP': 1.6773508787155151}\n",
    "c:\\Users\\Acer\\.conda\\envs\\nlp-env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35901bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000, Train Loss: 0.849301, Val Loss: 0.837252\n",
      "Epoch 20/20000, Train Loss: 0.211235, Val Loss: 0.182509\n",
      "Epoch 40/20000, Train Loss: 0.073330, Val Loss: 0.037957\n",
      "Epoch 60/20000, Train Loss: 0.042940, Val Loss: 0.016386\n",
      "Epoch 80/20000, Train Loss: 0.032759, Val Loss: 0.008358\n",
      "Epoch 100/20000, Train Loss: 0.025403, Val Loss: 0.005113\n",
      "Epoch 120/20000, Train Loss: 0.023227, Val Loss: 0.003281\n",
      "Epoch 140/20000, Train Loss: 0.021731, Val Loss: 0.002132\n",
      "Epoch 160/20000, Train Loss: 0.019065, Val Loss: 0.001535\n",
      "Epoch 180/20000, Train Loss: 0.016928, Val Loss: 0.001243\n",
      "Epoch 200/20000, Train Loss: 0.016768, Val Loss: 0.001089\n",
      "Epoch 220/20000, Train Loss: 0.016447, Val Loss: 0.000943\n",
      "Epoch 240/20000, Train Loss: 0.014431, Val Loss: 0.000734\n",
      "Epoch 260/20000, Train Loss: 0.014701, Val Loss: 0.000869\n",
      "Epoch 280/20000, Train Loss: 0.014219, Val Loss: 0.000962\n",
      "Epoch 300/20000, Train Loss: 0.013764, Val Loss: 0.000790\n",
      "Epoch 320/20000, Train Loss: 0.013847, Val Loss: 0.000489\n",
      "Epoch 340/20000, Train Loss: 0.012781, Val Loss: 0.000642\n",
      "Epoch 360/20000, Train Loss: 0.012180, Val Loss: 0.000719\n",
      "Epoch 380/20000, Train Loss: 0.011865, Val Loss: 0.000590\n",
      "Epoch 400/20000, Train Loss: 0.011411, Val Loss: 0.000657\n",
      "Epoch 420/20000, Train Loss: 0.011321, Val Loss: 0.000689\n",
      "Epoch 440/20000, Train Loss: 0.010418, Val Loss: 0.000603\n",
      "Epoch 460/20000, Train Loss: 0.010591, Val Loss: 0.000630\n",
      "Epoch 480/20000, Train Loss: 0.010400, Val Loss: 0.000447\n",
      "Epoch 500/20000, Train Loss: 0.010627, Val Loss: 0.000554\n",
      "Epoch 520/20000, Train Loss: 0.009533, Val Loss: 0.000517\n",
      "Epoch 540/20000, Train Loss: 0.010362, Val Loss: 0.000573\n",
      "Epoch 560/20000, Train Loss: 0.009677, Val Loss: 0.000577\n",
      "Epoch 580/20000, Train Loss: 0.009276, Val Loss: 0.000453\n",
      "Epoch 600/20000, Train Loss: 0.009654, Val Loss: 0.000395\n",
      "Epoch 620/20000, Train Loss: 0.009791, Val Loss: 0.000650\n",
      "Epoch 640/20000, Train Loss: 0.009237, Val Loss: 0.000320\n",
      "Epoch 660/20000, Train Loss: 0.008741, Val Loss: 0.000471\n",
      "Epoch 680/20000, Train Loss: 0.009415, Val Loss: 0.000307\n",
      "Epoch 700/20000, Train Loss: 0.008601, Val Loss: 0.000484\n",
      "Epoch 720/20000, Train Loss: 0.009313, Val Loss: 0.000650\n",
      "Epoch 740/20000, Train Loss: 0.008271, Val Loss: 0.000815\n",
      "Epoch 760/20000, Train Loss: 0.008239, Val Loss: 0.000541\n",
      "Epoch 780/20000, Train Loss: 0.008520, Val Loss: 0.000718\n",
      "Epoch 800/20000, Train Loss: 0.008757, Val Loss: 0.000689\n",
      "Epoch 820/20000, Train Loss: 0.008151, Val Loss: 0.000818\n",
      "Epoch 840/20000, Train Loss: 0.008374, Val Loss: 0.000370\n",
      "Epoch 860/20000, Train Loss: 0.008308, Val Loss: 0.000535\n",
      "Epoch 880/20000, Train Loss: 0.008222, Val Loss: 0.000357\n",
      "Epoch 900/20000, Train Loss: 0.008170, Val Loss: 0.000674\n",
      "Epoch 920/20000, Train Loss: 0.008092, Val Loss: 0.000292\n",
      "Epoch 940/20000, Train Loss: 0.007384, Val Loss: 0.000597\n",
      "Epoch 960/20000, Train Loss: 0.007725, Val Loss: 0.000566\n",
      "Epoch 980/20000, Train Loss: 0.008080, Val Loss: 0.000383\n",
      "Epoch 1000/20000, Train Loss: 0.007918, Val Loss: 0.000432\n",
      "Epoch 1020/20000, Train Loss: 0.007270, Val Loss: 0.000451\n",
      "Epoch 1040/20000, Train Loss: 0.007306, Val Loss: 0.000264\n",
      "Epoch 1060/20000, Train Loss: 0.007221, Val Loss: 0.000283\n",
      "Epoch 1080/20000, Train Loss: 0.007510, Val Loss: 0.000326\n",
      "Epoch 1100/20000, Train Loss: 0.007348, Val Loss: 0.000229\n",
      "Epoch 1120/20000, Train Loss: 0.006643, Val Loss: 0.000205\n",
      "Epoch 1140/20000, Train Loss: 0.007662, Val Loss: 0.000363\n",
      "Epoch 1160/20000, Train Loss: 0.007281, Val Loss: 0.000268\n",
      "Epoch 1180/20000, Train Loss: 0.006944, Val Loss: 0.000461\n",
      "Epoch 1200/20000, Train Loss: 0.006675, Val Loss: 0.000365\n",
      "Epoch 1220/20000, Train Loss: 0.007033, Val Loss: 0.000308\n",
      "Epoch 1240/20000, Train Loss: 0.007162, Val Loss: 0.000580\n",
      "Epoch 1260/20000, Train Loss: 0.007193, Val Loss: 0.000313\n",
      "Epoch 1280/20000, Train Loss: 0.006804, Val Loss: 0.000341\n",
      "Epoch 1300/20000, Train Loss: 0.007320, Val Loss: 0.000827\n",
      "Epoch 1320/20000, Train Loss: 0.006921, Val Loss: 0.000537\n",
      "Epoch 1340/20000, Train Loss: 0.006672, Val Loss: 0.000425\n",
      "Epoch 1360/20000, Train Loss: 0.006953, Val Loss: 0.000478\n",
      "Epoch 1380/20000, Train Loss: 0.006406, Val Loss: 0.000311\n",
      "Epoch 1400/20000, Train Loss: 0.007124, Val Loss: 0.000314\n",
      "Epoch 1420/20000, Train Loss: 0.006501, Val Loss: 0.000776\n",
      "Epoch 1440/20000, Train Loss: 0.006823, Val Loss: 0.000879\n",
      "Epoch 1460/20000, Train Loss: 0.006309, Val Loss: 0.000172\n",
      "Epoch 1480/20000, Train Loss: 0.006024, Val Loss: 0.000287\n",
      "Epoch 1500/20000, Train Loss: 0.006196, Val Loss: 0.000293\n",
      "Epoch 1520/20000, Train Loss: 0.006417, Val Loss: 0.000324\n",
      "Epoch 1540/20000, Train Loss: 0.006434, Val Loss: 0.000379\n",
      "Epoch 1560/20000, Train Loss: 0.006367, Val Loss: 0.000263\n",
      "Epoch 1580/20000, Train Loss: 0.006226, Val Loss: 0.000312\n",
      "Epoch 1600/20000, Train Loss: 0.006079, Val Loss: 0.000466\n",
      "Epoch 1620/20000, Train Loss: 0.006593, Val Loss: 0.000338\n",
      "Epoch 1640/20000, Train Loss: 0.006164, Val Loss: 0.000168\n",
      "Epoch 1660/20000, Train Loss: 0.005766, Val Loss: 0.000574\n",
      "Epoch 1680/20000, Train Loss: 0.006085, Val Loss: 0.000619\n",
      "Epoch 1700/20000, Train Loss: 0.005944, Val Loss: 0.000352\n",
      "Epoch 1720/20000, Train Loss: 0.006422, Val Loss: 0.000301\n",
      "Epoch 1740/20000, Train Loss: 0.005855, Val Loss: 0.000478\n",
      "Epoch 1760/20000, Train Loss: 0.005534, Val Loss: 0.000365\n",
      "Epoch 1780/20000, Train Loss: 0.005868, Val Loss: 0.000235\n",
      "Epoch 1800/20000, Train Loss: 0.005632, Val Loss: 0.000799\n",
      "Epoch 1820/20000, Train Loss: 0.005694, Val Loss: 0.000212\n",
      "Epoch 1840/20000, Train Loss: 0.005768, Val Loss: 0.000384\n",
      "Epoch 1860/20000, Train Loss: 0.006076, Val Loss: 0.000288\n",
      "Epoch 1880/20000, Train Loss: 0.005760, Val Loss: 0.000524\n",
      "Epoch 1900/20000, Train Loss: 0.005807, Val Loss: 0.000238\n",
      "Epoch 1920/20000, Train Loss: 0.005519, Val Loss: 0.000121\n",
      "Epoch 1940/20000, Train Loss: 0.005636, Val Loss: 0.000348\n",
      "Epoch 1960/20000, Train Loss: 0.005362, Val Loss: 0.000324\n",
      "Epoch 1980/20000, Train Loss: 0.005382, Val Loss: 0.000256\n",
      "Epoch 2000/20000, Train Loss: 0.005887, Val Loss: 0.000457\n",
      "Epoch 2020/20000, Train Loss: 0.005306, Val Loss: 0.000660\n",
      "Epoch 2040/20000, Train Loss: 0.005888, Val Loss: 0.000403\n",
      "Epoch 2060/20000, Train Loss: 0.005311, Val Loss: 0.000175\n",
      "Epoch 2080/20000, Train Loss: 0.005405, Val Loss: 0.000302\n",
      "Epoch 2100/20000, Train Loss: 0.005538, Val Loss: 0.000105\n",
      "Epoch 2120/20000, Train Loss: 0.005480, Val Loss: 0.000237\n",
      "Epoch 2140/20000, Train Loss: 0.005379, Val Loss: 0.000594\n",
      "Epoch 2160/20000, Train Loss: 0.005197, Val Loss: 0.000177\n",
      "Epoch 2180/20000, Train Loss: 0.005298, Val Loss: 0.000356\n",
      "Epoch 2200/20000, Train Loss: 0.005818, Val Loss: 0.000247\n",
      "Epoch 2220/20000, Train Loss: 0.005131, Val Loss: 0.000475\n",
      "Epoch 2240/20000, Train Loss: 0.004800, Val Loss: 0.000135\n",
      "Epoch 2260/20000, Train Loss: 0.005399, Val Loss: 0.000125\n",
      "Epoch 2280/20000, Train Loss: 0.005607, Val Loss: 0.000195\n",
      "Epoch 2300/20000, Train Loss: 0.005224, Val Loss: 0.000319\n",
      "Epoch 2320/20000, Train Loss: 0.005425, Val Loss: 0.000146\n",
      "Epoch 2340/20000, Train Loss: 0.005039, Val Loss: 0.000318\n",
      "Epoch 2360/20000, Train Loss: 0.005494, Val Loss: 0.000335\n",
      "Epoch 2380/20000, Train Loss: 0.005155, Val Loss: 0.000255\n",
      "Epoch 2400/20000, Train Loss: 0.005319, Val Loss: 0.000291\n",
      "Epoch 2420/20000, Train Loss: 0.005431, Val Loss: 0.000108\n",
      "Epoch 2440/20000, Train Loss: 0.005193, Val Loss: 0.000291\n",
      "Epoch 2460/20000, Train Loss: 0.004920, Val Loss: 0.000380\n",
      "Epoch 2480/20000, Train Loss: 0.005202, Val Loss: 0.000137\n",
      "Epoch 2500/20000, Train Loss: 0.005056, Val Loss: 0.000385\n",
      "Epoch 2520/20000, Train Loss: 0.004983, Val Loss: 0.000124\n",
      "Epoch 2540/20000, Train Loss: 0.005172, Val Loss: 0.000133\n",
      "Epoch 2560/20000, Train Loss: 0.005169, Val Loss: 0.000148\n",
      "Epoch 2580/20000, Train Loss: 0.005126, Val Loss: 0.000073\n",
      "Epoch 2600/20000, Train Loss: 0.004954, Val Loss: 0.000385\n",
      "Epoch 2620/20000, Train Loss: 0.005100, Val Loss: 0.000095\n",
      "Epoch 2640/20000, Train Loss: 0.004939, Val Loss: 0.000076\n",
      "Epoch 2660/20000, Train Loss: 0.005065, Val Loss: 0.000098\n",
      "Epoch 2680/20000, Train Loss: 0.004889, Val Loss: 0.000150\n",
      "Epoch 2700/20000, Train Loss: 0.004847, Val Loss: 0.000185\n",
      "Epoch 2720/20000, Train Loss: 0.005043, Val Loss: 0.000124\n",
      "Epoch 2740/20000, Train Loss: 0.005137, Val Loss: 0.000131\n",
      "Epoch 2760/20000, Train Loss: 0.005127, Val Loss: 0.000226\n",
      "Epoch 2780/20000, Train Loss: 0.004915, Val Loss: 0.000127\n",
      "Epoch 2800/20000, Train Loss: 0.004906, Val Loss: 0.000154\n",
      "Epoch 2820/20000, Train Loss: 0.004643, Val Loss: 0.000229\n",
      "Epoch 2840/20000, Train Loss: 0.005079, Val Loss: 0.000081\n",
      "Epoch 2860/20000, Train Loss: 0.005090, Val Loss: 0.000161\n",
      "Epoch 2880/20000, Train Loss: 0.004850, Val Loss: 0.000240\n",
      "Epoch 2900/20000, Train Loss: 0.004951, Val Loss: 0.000173\n",
      "Epoch 2920/20000, Train Loss: 0.004970, Val Loss: 0.000137\n",
      "Epoch 2940/20000, Train Loss: 0.004902, Val Loss: 0.000165\n",
      "Epoch 2960/20000, Train Loss: 0.004698, Val Loss: 0.000211\n",
      "Epoch 2980/20000, Train Loss: 0.004812, Val Loss: 0.000118\n",
      "Epoch 3000/20000, Train Loss: 0.004688, Val Loss: 0.000237\n",
      "Epoch 3020/20000, Train Loss: 0.004395, Val Loss: 0.000207\n",
      "Epoch 3040/20000, Train Loss: 0.004688, Val Loss: 0.000232\n",
      "Epoch 3060/20000, Train Loss: 0.004955, Val Loss: 0.000127\n",
      "Epoch 3080/20000, Train Loss: 0.004651, Val Loss: 0.000197\n",
      "Epoch 3100/20000, Train Loss: 0.005314, Val Loss: 0.000105\n",
      "Epoch 3120/20000, Train Loss: 0.004849, Val Loss: 0.000410\n",
      "Epoch 3140/20000, Train Loss: 0.004642, Val Loss: 0.000273\n",
      "Epoch 3160/20000, Train Loss: 0.004596, Val Loss: 0.000214\n",
      "Epoch 3180/20000, Train Loss: 0.004620, Val Loss: 0.000152\n",
      "Epoch 3200/20000, Train Loss: 0.004540, Val Loss: 0.000151\n",
      "Epoch 3220/20000, Train Loss: 0.004978, Val Loss: 0.000130\n",
      "Epoch 3240/20000, Train Loss: 0.004487, Val Loss: 0.000203\n",
      "Epoch 3260/20000, Train Loss: 0.004617, Val Loss: 0.000140\n",
      "Epoch 3280/20000, Train Loss: 0.004663, Val Loss: 0.000118\n",
      "Epoch 3300/20000, Train Loss: 0.004514, Val Loss: 0.000165\n",
      "Epoch 3320/20000, Train Loss: 0.004512, Val Loss: 0.000079\n",
      "Epoch 3340/20000, Train Loss: 0.004506, Val Loss: 0.000131\n",
      "Epoch 3360/20000, Train Loss: 0.004648, Val Loss: 0.000226\n",
      "Epoch 3380/20000, Train Loss: 0.004446, Val Loss: 0.000181\n",
      "Epoch 3400/20000, Train Loss: 0.004518, Val Loss: 0.000077\n",
      "Epoch 3420/20000, Train Loss: 0.004509, Val Loss: 0.000147\n",
      "Epoch 3440/20000, Train Loss: 0.004382, Val Loss: 0.000388\n",
      "Epoch 3460/20000, Train Loss: 0.004868, Val Loss: 0.000163\n",
      "Epoch 3480/20000, Train Loss: 0.004518, Val Loss: 0.000108\n",
      "Epoch 3500/20000, Train Loss: 0.004346, Val Loss: 0.000094\n",
      "Epoch 3520/20000, Train Loss: 0.004479, Val Loss: 0.000185\n",
      "Epoch 3540/20000, Train Loss: 0.004704, Val Loss: 0.000157\n",
      "Epoch 3560/20000, Train Loss: 0.004472, Val Loss: 0.000101\n",
      "Epoch 3580/20000, Train Loss: 0.004589, Val Loss: 0.000141\n",
      "Epoch 3600/20000, Train Loss: 0.004534, Val Loss: 0.000349\n",
      "Epoch 3620/20000, Train Loss: 0.004559, Val Loss: 0.000149\n",
      "Epoch 3640/20000, Train Loss: 0.004318, Val Loss: 0.000147\n",
      "Epoch 3660/20000, Train Loss: 0.004850, Val Loss: 0.000117\n",
      "Epoch 3680/20000, Train Loss: 0.004664, Val Loss: 0.000130\n",
      "Epoch 3700/20000, Train Loss: 0.004462, Val Loss: 0.000151\n",
      "Epoch 3720/20000, Train Loss: 0.004631, Val Loss: 0.000128\n",
      "Epoch 3740/20000, Train Loss: 0.004553, Val Loss: 0.000116\n",
      "Epoch 3760/20000, Train Loss: 0.004277, Val Loss: 0.000120\n",
      "Epoch 3780/20000, Train Loss: 0.004287, Val Loss: 0.000250\n",
      "Epoch 3800/20000, Train Loss: 0.004625, Val Loss: 0.000165\n",
      "Epoch 3820/20000, Train Loss: 0.004317, Val Loss: 0.000111\n",
      "Epoch 3840/20000, Train Loss: 0.004580, Val Loss: 0.000068\n",
      "Epoch 3860/20000, Train Loss: 0.004289, Val Loss: 0.000247\n",
      "Epoch 3880/20000, Train Loss: 0.004199, Val Loss: 0.000128\n",
      "Epoch 3900/20000, Train Loss: 0.004468, Val Loss: 0.000055\n",
      "Epoch 3920/20000, Train Loss: 0.004552, Val Loss: 0.000257\n",
      "Epoch 3940/20000, Train Loss: 0.004166, Val Loss: 0.000098\n",
      "Epoch 3960/20000, Train Loss: 0.004590, Val Loss: 0.000128\n",
      "Epoch 3980/20000, Train Loss: 0.004666, Val Loss: 0.000089\n",
      "Epoch 4000/20000, Train Loss: 0.004525, Val Loss: 0.000174\n",
      "Epoch 4020/20000, Train Loss: 0.004467, Val Loss: 0.000161\n",
      "Epoch 4040/20000, Train Loss: 0.004616, Val Loss: 0.000115\n",
      "Epoch 4060/20000, Train Loss: 0.004525, Val Loss: 0.000059\n",
      "Epoch 4080/20000, Train Loss: 0.004147, Val Loss: 0.000161\n",
      "Epoch 4100/20000, Train Loss: 0.004494, Val Loss: 0.000165\n",
      "Epoch 4120/20000, Train Loss: 0.004772, Val Loss: 0.000107\n",
      "Epoch 4140/20000, Train Loss: 0.004410, Val Loss: 0.000171\n",
      "Epoch 4160/20000, Train Loss: 0.004256, Val Loss: 0.000113\n",
      "Epoch 4180/20000, Train Loss: 0.004411, Val Loss: 0.000087\n",
      "Epoch 4200/20000, Train Loss: 0.004440, Val Loss: 0.000198\n",
      "Epoch 4220/20000, Train Loss: 0.004239, Val Loss: 0.000169\n",
      "Epoch 4240/20000, Train Loss: 0.004591, Val Loss: 0.000109\n",
      "Epoch 4260/20000, Train Loss: 0.004387, Val Loss: 0.000093\n",
      "Epoch 4280/20000, Train Loss: 0.004432, Val Loss: 0.000053\n",
      "Epoch 4300/20000, Train Loss: 0.004373, Val Loss: 0.000158\n",
      "Epoch 4320/20000, Train Loss: 0.004418, Val Loss: 0.000102\n",
      "Epoch 4340/20000, Train Loss: 0.004304, Val Loss: 0.000159\n",
      "Epoch 4360/20000, Train Loss: 0.004497, Val Loss: 0.000102\n",
      "Epoch 4380/20000, Train Loss: 0.004338, Val Loss: 0.000088\n",
      "Epoch 4400/20000, Train Loss: 0.004346, Val Loss: 0.000186\n",
      "Epoch 4420/20000, Train Loss: 0.004289, Val Loss: 0.000179\n",
      "Epoch 4440/20000, Train Loss: 0.004346, Val Loss: 0.000080\n",
      "Epoch 4460/20000, Train Loss: 0.004234, Val Loss: 0.000134\n",
      "Epoch 4480/20000, Train Loss: 0.004204, Val Loss: 0.000098\n",
      "Epoch 4500/20000, Train Loss: 0.004541, Val Loss: 0.000198\n",
      "Epoch 4520/20000, Train Loss: 0.004236, Val Loss: 0.000082\n",
      "Epoch 4540/20000, Train Loss: 0.004400, Val Loss: 0.000078\n",
      "Epoch 4560/20000, Train Loss: 0.004175, Val Loss: 0.000181\n",
      "Epoch 4580/20000, Train Loss: 0.004354, Val Loss: 0.000089\n",
      "Epoch 4600/20000, Train Loss: 0.004481, Val Loss: 0.000107\n",
      "Epoch 4620/20000, Train Loss: 0.004179, Val Loss: 0.000231\n",
      "Epoch 4640/20000, Train Loss: 0.003942, Val Loss: 0.000068\n",
      "Epoch 4660/20000, Train Loss: 0.004214, Val Loss: 0.000102\n",
      "Epoch 4680/20000, Train Loss: 0.004170, Val Loss: 0.000061\n",
      "Epoch 4700/20000, Train Loss: 0.004100, Val Loss: 0.000103\n",
      "Epoch 4720/20000, Train Loss: 0.004262, Val Loss: 0.000080\n",
      "Epoch 4740/20000, Train Loss: 0.004335, Val Loss: 0.000086\n",
      "Epoch 4760/20000, Train Loss: 0.004655, Val Loss: 0.000146\n",
      "Epoch 4780/20000, Train Loss: 0.004437, Val Loss: 0.000199\n",
      "Epoch 4800/20000, Train Loss: 0.004310, Val Loss: 0.000300\n",
      "Epoch 4820/20000, Train Loss: 0.004093, Val Loss: 0.000162\n",
      "Epoch 4840/20000, Train Loss: 0.004501, Val Loss: 0.000184\n",
      "Epoch 4860/20000, Train Loss: 0.004135, Val Loss: 0.000103\n",
      "Epoch 4880/20000, Train Loss: 0.004171, Val Loss: 0.000072\n",
      "Epoch 4900/20000, Train Loss: 0.004410, Val Loss: 0.000158\n",
      "Epoch 4920/20000, Train Loss: 0.004592, Val Loss: 0.000120\n",
      "Epoch 4940/20000, Train Loss: 0.004318, Val Loss: 0.000108\n",
      "Epoch 4960/20000, Train Loss: 0.004104, Val Loss: 0.000116\n",
      "Epoch 4980/20000, Train Loss: 0.004128, Val Loss: 0.000121\n",
      "Epoch 5000/20000, Train Loss: 0.004313, Val Loss: 0.000067\n",
      "Epoch 5020/20000, Train Loss: 0.004532, Val Loss: 0.000116\n",
      "Epoch 5040/20000, Train Loss: 0.004317, Val Loss: 0.000101\n",
      "Epoch 5060/20000, Train Loss: 0.004232, Val Loss: 0.000175\n",
      "Epoch 5080/20000, Train Loss: 0.004388, Val Loss: 0.000056\n",
      "Epoch 5100/20000, Train Loss: 0.004410, Val Loss: 0.000119\n",
      "Epoch 5120/20000, Train Loss: 0.004167, Val Loss: 0.000105\n",
      "Epoch 5140/20000, Train Loss: 0.004270, Val Loss: 0.000068\n",
      "Epoch 5160/20000, Train Loss: 0.004216, Val Loss: 0.000195\n",
      "Epoch 5180/20000, Train Loss: 0.004350, Val Loss: 0.000092\n",
      "Epoch 5200/20000, Train Loss: 0.004161, Val Loss: 0.000076\n",
      "Epoch 5220/20000, Train Loss: 0.004125, Val Loss: 0.000098\n",
      "Epoch 5240/20000, Train Loss: 0.004136, Val Loss: 0.000056\n",
      "Epoch 5260/20000, Train Loss: 0.004278, Val Loss: 0.000181\n",
      "Epoch 5280/20000, Train Loss: 0.004182, Val Loss: 0.000107\n",
      "Epoch 5300/20000, Train Loss: 0.004398, Val Loss: 0.000090\n",
      "Epoch 5320/20000, Train Loss: 0.004303, Val Loss: 0.000125\n",
      "Epoch 5340/20000, Train Loss: 0.004156, Val Loss: 0.000239\n",
      "Epoch 5360/20000, Train Loss: 0.004232, Val Loss: 0.000183\n",
      "Epoch 5380/20000, Train Loss: 0.004341, Val Loss: 0.000138\n",
      "Epoch 5400/20000, Train Loss: 0.004376, Val Loss: 0.000126\n",
      "Epoch 5420/20000, Train Loss: 0.004151, Val Loss: 0.000161\n",
      "Epoch 5440/20000, Train Loss: 0.004204, Val Loss: 0.000189\n",
      "Epoch 5460/20000, Train Loss: 0.004124, Val Loss: 0.000109\n",
      "Epoch 5480/20000, Train Loss: 0.004314, Val Loss: 0.000153\n",
      "Epoch 5500/20000, Train Loss: 0.004044, Val Loss: 0.000072\n",
      "Epoch 5520/20000, Train Loss: 0.004400, Val Loss: 0.000128\n",
      "Epoch 5540/20000, Train Loss: 0.004268, Val Loss: 0.000084\n",
      "Epoch 5560/20000, Train Loss: 0.004380, Val Loss: 0.000127\n",
      "Epoch 5580/20000, Train Loss: 0.004378, Val Loss: 0.000105\n",
      "Epoch 5600/20000, Train Loss: 0.004047, Val Loss: 0.000196\n",
      "Epoch 5620/20000, Train Loss: 0.004372, Val Loss: 0.000116\n",
      "Epoch 5640/20000, Train Loss: 0.004262, Val Loss: 0.000084\n",
      "Epoch 5660/20000, Train Loss: 0.004339, Val Loss: 0.000129\n",
      "Epoch 5680/20000, Train Loss: 0.004432, Val Loss: 0.000077\n",
      "Epoch 5700/20000, Train Loss: 0.004074, Val Loss: 0.000145\n",
      "Epoch 5720/20000, Train Loss: 0.004060, Val Loss: 0.000140\n",
      "Epoch 5740/20000, Train Loss: 0.004077, Val Loss: 0.000120\n",
      "Epoch 5760/20000, Train Loss: 0.004211, Val Loss: 0.000120\n",
      "Epoch 5780/20000, Train Loss: 0.004257, Val Loss: 0.000121\n",
      "Epoch 5800/20000, Train Loss: 0.004422, Val Loss: 0.000190\n",
      "Epoch 5820/20000, Train Loss: 0.004108, Val Loss: 0.000104\n",
      "Epoch 5840/20000, Train Loss: 0.004307, Val Loss: 0.000124\n",
      "Epoch 5860/20000, Train Loss: 0.004495, Val Loss: 0.000102\n",
      "Epoch 5880/20000, Train Loss: 0.004082, Val Loss: 0.000104\n",
      "Epoch 5900/20000, Train Loss: 0.004297, Val Loss: 0.000133\n",
      "Epoch 5920/20000, Train Loss: 0.004139, Val Loss: 0.000088\n",
      "Epoch 5940/20000, Train Loss: 0.004051, Val Loss: 0.000094\n",
      "Epoch 5960/20000, Train Loss: 0.004356, Val Loss: 0.000069\n",
      "Epoch 5980/20000, Train Loss: 0.004090, Val Loss: 0.000104\n",
      "Epoch 6000/20000, Train Loss: 0.004166, Val Loss: 0.000103\n",
      "Epoch 6020/20000, Train Loss: 0.004055, Val Loss: 0.000094\n",
      "Epoch 6040/20000, Train Loss: 0.004072, Val Loss: 0.000073\n",
      "Epoch 6060/20000, Train Loss: 0.003974, Val Loss: 0.000104\n",
      "Epoch 6080/20000, Train Loss: 0.004305, Val Loss: 0.000117\n",
      "Epoch 6100/20000, Train Loss: 0.004272, Val Loss: 0.000109\n",
      "Epoch 6120/20000, Train Loss: 0.004196, Val Loss: 0.000091\n",
      "Epoch 6140/20000, Train Loss: 0.004401, Val Loss: 0.000174\n",
      "Epoch 6160/20000, Train Loss: 0.004220, Val Loss: 0.000126\n",
      "Epoch 6180/20000, Train Loss: 0.004027, Val Loss: 0.000050\n",
      "Epoch 6200/20000, Train Loss: 0.004094, Val Loss: 0.000129\n",
      "Epoch 6220/20000, Train Loss: 0.004202, Val Loss: 0.000191\n",
      "Epoch 6240/20000, Train Loss: 0.003693, Val Loss: 0.000093\n",
      "Epoch 6260/20000, Train Loss: 0.004237, Val Loss: 0.000073\n",
      "Epoch 6280/20000, Train Loss: 0.004208, Val Loss: 0.000144\n",
      "Epoch 6300/20000, Train Loss: 0.003894, Val Loss: 0.000150\n",
      "Epoch 6320/20000, Train Loss: 0.004119, Val Loss: 0.000080\n",
      "Epoch 6340/20000, Train Loss: 0.004079, Val Loss: 0.000138\n",
      "Epoch 6360/20000, Train Loss: 0.004107, Val Loss: 0.000066\n",
      "Epoch 6380/20000, Train Loss: 0.004090, Val Loss: 0.000249\n",
      "Epoch 6400/20000, Train Loss: 0.004472, Val Loss: 0.000080\n",
      "Epoch 6420/20000, Train Loss: 0.004226, Val Loss: 0.000107\n",
      "Epoch 6440/20000, Train Loss: 0.004284, Val Loss: 0.000086\n",
      "Epoch 6460/20000, Train Loss: 0.004305, Val Loss: 0.000063\n",
      "Epoch 6480/20000, Train Loss: 0.004132, Val Loss: 0.000226\n",
      "Epoch 6500/20000, Train Loss: 0.004155, Val Loss: 0.000087\n",
      "Epoch 6520/20000, Train Loss: 0.004058, Val Loss: 0.000124\n",
      "Epoch 6540/20000, Train Loss: 0.004351, Val Loss: 0.000201\n",
      "Epoch 6560/20000, Train Loss: 0.004037, Val Loss: 0.000090\n",
      "Epoch 6580/20000, Train Loss: 0.004019, Val Loss: 0.000289\n",
      "Epoch 6600/20000, Train Loss: 0.004067, Val Loss: 0.000081\n",
      "Epoch 6620/20000, Train Loss: 0.004080, Val Loss: 0.000124\n",
      "Epoch 6640/20000, Train Loss: 0.004209, Val Loss: 0.000049\n",
      "Epoch 6660/20000, Train Loss: 0.004267, Val Loss: 0.000093\n",
      "Epoch 6680/20000, Train Loss: 0.004327, Val Loss: 0.000084\n",
      "Epoch 6700/20000, Train Loss: 0.003992, Val Loss: 0.000070\n",
      "Epoch 6720/20000, Train Loss: 0.004171, Val Loss: 0.000206\n",
      "Epoch 6740/20000, Train Loss: 0.004122, Val Loss: 0.000115\n",
      "Epoch 6760/20000, Train Loss: 0.004225, Val Loss: 0.000081\n",
      "Epoch 6780/20000, Train Loss: 0.004116, Val Loss: 0.000143\n",
      "Epoch 6800/20000, Train Loss: 0.004124, Val Loss: 0.000065\n",
      "Epoch 6820/20000, Train Loss: 0.004271, Val Loss: 0.000084\n",
      "Epoch 6840/20000, Train Loss: 0.004125, Val Loss: 0.000092\n",
      "Epoch 6860/20000, Train Loss: 0.004243, Val Loss: 0.000157\n",
      "Epoch 6880/20000, Train Loss: 0.004181, Val Loss: 0.000099\n",
      "Epoch 6900/20000, Train Loss: 0.003873, Val Loss: 0.000090\n",
      "Epoch 6920/20000, Train Loss: 0.004111, Val Loss: 0.000111\n",
      "Epoch 6940/20000, Train Loss: 0.004106, Val Loss: 0.000142\n",
      "Epoch 6960/20000, Train Loss: 0.004099, Val Loss: 0.000118\n",
      "Epoch 6980/20000, Train Loss: 0.004018, Val Loss: 0.000062\n",
      "Epoch 7000/20000, Train Loss: 0.004318, Val Loss: 0.000127\n",
      "Epoch 7020/20000, Train Loss: 0.004085, Val Loss: 0.000169\n",
      "Epoch 7040/20000, Train Loss: 0.004220, Val Loss: 0.000100\n",
      "Epoch 7060/20000, Train Loss: 0.004217, Val Loss: 0.000116\n",
      "Epoch 7080/20000, Train Loss: 0.004416, Val Loss: 0.000114\n",
      "Epoch 7100/20000, Train Loss: 0.004092, Val Loss: 0.000078\n",
      "Epoch 7120/20000, Train Loss: 0.004225, Val Loss: 0.000104\n",
      "Epoch 7140/20000, Train Loss: 0.004195, Val Loss: 0.000061\n",
      "Epoch 7160/20000, Train Loss: 0.003931, Val Loss: 0.000069\n",
      "Epoch 7180/20000, Train Loss: 0.004052, Val Loss: 0.000058\n",
      "Epoch 7200/20000, Train Loss: 0.003790, Val Loss: 0.000059\n",
      "Epoch 7220/20000, Train Loss: 0.004005, Val Loss: 0.000090\n",
      "Epoch 7240/20000, Train Loss: 0.004139, Val Loss: 0.000059\n",
      "Epoch 7260/20000, Train Loss: 0.004183, Val Loss: 0.000140\n",
      "Epoch 7280/20000, Train Loss: 0.004108, Val Loss: 0.000218\n",
      "Epoch 7300/20000, Train Loss: 0.004124, Val Loss: 0.000186\n",
      "Epoch 7320/20000, Train Loss: 0.003837, Val Loss: 0.000152\n",
      "Epoch 7340/20000, Train Loss: 0.003998, Val Loss: 0.000085\n",
      "Epoch 7360/20000, Train Loss: 0.004180, Val Loss: 0.000097\n",
      "Epoch 7380/20000, Train Loss: 0.003805, Val Loss: 0.000092\n",
      "Epoch 7400/20000, Train Loss: 0.004033, Val Loss: 0.000073\n",
      "Epoch 7420/20000, Train Loss: 0.003964, Val Loss: 0.000163\n",
      "Epoch 7440/20000, Train Loss: 0.004059, Val Loss: 0.000186\n",
      "Epoch 7460/20000, Train Loss: 0.004110, Val Loss: 0.000150\n",
      "Epoch 7480/20000, Train Loss: 0.003971, Val Loss: 0.000096\n",
      "Epoch 7500/20000, Train Loss: 0.003958, Val Loss: 0.000063\n",
      "Epoch 7520/20000, Train Loss: 0.004369, Val Loss: 0.000074\n",
      "Epoch 7540/20000, Train Loss: 0.003904, Val Loss: 0.000088\n",
      "Epoch 7560/20000, Train Loss: 0.004174, Val Loss: 0.000089\n",
      "Epoch 7580/20000, Train Loss: 0.004188, Val Loss: 0.000153\n",
      "Epoch 7600/20000, Train Loss: 0.004233, Val Loss: 0.000179\n",
      "Epoch 7620/20000, Train Loss: 0.003632, Val Loss: 0.000139\n",
      "Epoch 7640/20000, Train Loss: 0.004279, Val Loss: 0.000092\n",
      "Epoch 7660/20000, Train Loss: 0.004120, Val Loss: 0.000122\n",
      "Epoch 7680/20000, Train Loss: 0.003957, Val Loss: 0.000175\n",
      "Epoch 7700/20000, Train Loss: 0.004014, Val Loss: 0.000125\n",
      "Epoch 7720/20000, Train Loss: 0.003704, Val Loss: 0.000089\n",
      "Epoch 7740/20000, Train Loss: 0.004266, Val Loss: 0.000085\n",
      "Epoch 7760/20000, Train Loss: 0.004030, Val Loss: 0.000284\n",
      "Epoch 7780/20000, Train Loss: 0.004223, Val Loss: 0.000234\n",
      "Epoch 7800/20000, Train Loss: 0.004038, Val Loss: 0.000063\n",
      "Epoch 7820/20000, Train Loss: 0.003940, Val Loss: 0.000108\n",
      "Epoch 7840/20000, Train Loss: 0.004026, Val Loss: 0.000093\n",
      "Epoch 7860/20000, Train Loss: 0.004026, Val Loss: 0.000130\n",
      "Epoch 7880/20000, Train Loss: 0.003879, Val Loss: 0.000103\n",
      "Epoch 7900/20000, Train Loss: 0.004146, Val Loss: 0.000110\n",
      "Epoch 7920/20000, Train Loss: 0.004165, Val Loss: 0.000203\n",
      "Epoch 7940/20000, Train Loss: 0.004066, Val Loss: 0.000112\n",
      "Epoch 7960/20000, Train Loss: 0.004189, Val Loss: 0.000069\n",
      "Epoch 7980/20000, Train Loss: 0.004016, Val Loss: 0.000088\n",
      "Epoch 8000/20000, Train Loss: 0.004102, Val Loss: 0.000117\n",
      "Epoch 8020/20000, Train Loss: 0.004253, Val Loss: 0.000227\n",
      "Epoch 8040/20000, Train Loss: 0.003839, Val Loss: 0.000211\n",
      "Epoch 8060/20000, Train Loss: 0.003999, Val Loss: 0.000110\n",
      "Epoch 8080/20000, Train Loss: 0.004217, Val Loss: 0.000135\n",
      "Epoch 8100/20000, Train Loss: 0.003718, Val Loss: 0.000111\n",
      "Epoch 8120/20000, Train Loss: 0.004109, Val Loss: 0.000086\n",
      "Epoch 8140/20000, Train Loss: 0.004083, Val Loss: 0.000090\n",
      "Epoch 8160/20000, Train Loss: 0.003924, Val Loss: 0.000079\n",
      "Epoch 8180/20000, Train Loss: 0.003924, Val Loss: 0.000133\n",
      "Epoch 8200/20000, Train Loss: 0.003849, Val Loss: 0.000092\n",
      "Epoch 8220/20000, Train Loss: 0.003946, Val Loss: 0.000093\n",
      "Epoch 8240/20000, Train Loss: 0.004074, Val Loss: 0.000137\n",
      "Epoch 8260/20000, Train Loss: 0.003984, Val Loss: 0.000096\n",
      "Epoch 8280/20000, Train Loss: 0.003871, Val Loss: 0.000101\n",
      "Epoch 8300/20000, Train Loss: 0.004053, Val Loss: 0.000073\n",
      "Epoch 8320/20000, Train Loss: 0.003989, Val Loss: 0.000085\n",
      "Epoch 8340/20000, Train Loss: 0.004072, Val Loss: 0.000076\n",
      "Epoch 8360/20000, Train Loss: 0.003896, Val Loss: 0.000074\n",
      "Epoch 8380/20000, Train Loss: 0.003993, Val Loss: 0.000168\n",
      "Epoch 8400/20000, Train Loss: 0.004060, Val Loss: 0.000136\n",
      "Epoch 8420/20000, Train Loss: 0.003784, Val Loss: 0.000122\n",
      "Epoch 8440/20000, Train Loss: 0.004117, Val Loss: 0.000097\n",
      "Epoch 8460/20000, Train Loss: 0.004014, Val Loss: 0.000097\n",
      "Epoch 8480/20000, Train Loss: 0.004033, Val Loss: 0.000077\n",
      "Epoch 8500/20000, Train Loss: 0.004153, Val Loss: 0.000082\n",
      "Epoch 8520/20000, Train Loss: 0.003981, Val Loss: 0.000053\n",
      "Epoch 8540/20000, Train Loss: 0.004318, Val Loss: 0.000100\n",
      "Epoch 8560/20000, Train Loss: 0.003715, Val Loss: 0.000134\n",
      "Epoch 8580/20000, Train Loss: 0.003735, Val Loss: 0.000098\n",
      "Epoch 8600/20000, Train Loss: 0.003994, Val Loss: 0.000249\n",
      "Epoch 8620/20000, Train Loss: 0.004137, Val Loss: 0.000185\n",
      "Epoch 8640/20000, Train Loss: 0.004136, Val Loss: 0.000096\n",
      "Epoch 8660/20000, Train Loss: 0.004179, Val Loss: 0.000105\n",
      "Epoch 8680/20000, Train Loss: 0.004056, Val Loss: 0.000148\n",
      "Epoch 8700/20000, Train Loss: 0.004080, Val Loss: 0.000119\n",
      "Epoch 8720/20000, Train Loss: 0.003903, Val Loss: 0.000067\n",
      "Epoch 8740/20000, Train Loss: 0.003691, Val Loss: 0.000124\n",
      "Epoch 8760/20000, Train Loss: 0.004010, Val Loss: 0.000092\n",
      "Epoch 8780/20000, Train Loss: 0.004132, Val Loss: 0.000106\n",
      "Epoch 8800/20000, Train Loss: 0.003745, Val Loss: 0.000081\n",
      "Epoch 8820/20000, Train Loss: 0.004033, Val Loss: 0.000042\n",
      "Epoch 8840/20000, Train Loss: 0.003747, Val Loss: 0.000073\n",
      "Epoch 8860/20000, Train Loss: 0.004066, Val Loss: 0.000067\n",
      "Epoch 8880/20000, Train Loss: 0.004019, Val Loss: 0.000098\n",
      "Epoch 8900/20000, Train Loss: 0.003743, Val Loss: 0.000112\n",
      "Epoch 8920/20000, Train Loss: 0.003927, Val Loss: 0.000056\n",
      "Epoch 8940/20000, Train Loss: 0.004136, Val Loss: 0.000062\n",
      "Epoch 8960/20000, Train Loss: 0.004201, Val Loss: 0.000075\n",
      "Epoch 8980/20000, Train Loss: 0.003898, Val Loss: 0.000065\n",
      "Epoch 9000/20000, Train Loss: 0.003907, Val Loss: 0.000124\n",
      "Epoch 9020/20000, Train Loss: 0.004095, Val Loss: 0.000132\n",
      "Epoch 9040/20000, Train Loss: 0.004088, Val Loss: 0.000072\n",
      "Epoch 9060/20000, Train Loss: 0.003884, Val Loss: 0.000139\n",
      "Epoch 9080/20000, Train Loss: 0.004199, Val Loss: 0.000146\n",
      "Epoch 9100/20000, Train Loss: 0.004045, Val Loss: 0.000098\n",
      "Epoch 9120/20000, Train Loss: 0.004054, Val Loss: 0.000185\n",
      "Epoch 9140/20000, Train Loss: 0.004057, Val Loss: 0.000091\n",
      "Epoch 9160/20000, Train Loss: 0.004253, Val Loss: 0.000114\n",
      "Epoch 9180/20000, Train Loss: 0.003861, Val Loss: 0.000116\n",
      "Epoch 9200/20000, Train Loss: 0.003878, Val Loss: 0.000057\n",
      "Epoch 9220/20000, Train Loss: 0.004219, Val Loss: 0.000091\n",
      "Epoch 9240/20000, Train Loss: 0.004134, Val Loss: 0.000122\n",
      "Epoch 9260/20000, Train Loss: 0.003907, Val Loss: 0.000062\n",
      "Epoch 9280/20000, Train Loss: 0.003913, Val Loss: 0.000101\n",
      "Epoch 9300/20000, Train Loss: 0.003741, Val Loss: 0.000101\n",
      "Epoch 9320/20000, Train Loss: 0.003915, Val Loss: 0.000083\n",
      "Epoch 9340/20000, Train Loss: 0.003929, Val Loss: 0.000134\n",
      "Epoch 9360/20000, Train Loss: 0.003923, Val Loss: 0.000167\n",
      "Epoch 9380/20000, Train Loss: 0.004015, Val Loss: 0.000238\n",
      "Epoch 9400/20000, Train Loss: 0.003972, Val Loss: 0.000053\n",
      "Epoch 9420/20000, Train Loss: 0.004059, Val Loss: 0.000083\n",
      "Epoch 9440/20000, Train Loss: 0.004017, Val Loss: 0.000112\n",
      "Epoch 9460/20000, Train Loss: 0.003938, Val Loss: 0.000071\n",
      "Epoch 9480/20000, Train Loss: 0.003844, Val Loss: 0.000122\n",
      "Epoch 9500/20000, Train Loss: 0.003616, Val Loss: 0.000088\n",
      "Epoch 9520/20000, Train Loss: 0.003917, Val Loss: 0.000237\n",
      "Epoch 9540/20000, Train Loss: 0.004119, Val Loss: 0.000090\n",
      "Epoch 9560/20000, Train Loss: 0.004008, Val Loss: 0.000103\n",
      "Epoch 9580/20000, Train Loss: 0.003740, Val Loss: 0.000077\n",
      "Epoch 9600/20000, Train Loss: 0.003975, Val Loss: 0.000051\n",
      "Epoch 9620/20000, Train Loss: 0.004000, Val Loss: 0.000128\n",
      "Epoch 9640/20000, Train Loss: 0.003877, Val Loss: 0.000078\n",
      "Epoch 9660/20000, Train Loss: 0.004043, Val Loss: 0.000041\n",
      "Epoch 9680/20000, Train Loss: 0.004258, Val Loss: 0.000072\n",
      "Epoch 9700/20000, Train Loss: 0.003998, Val Loss: 0.000170\n",
      "Epoch 9720/20000, Train Loss: 0.003996, Val Loss: 0.000128\n",
      "Epoch 9740/20000, Train Loss: 0.003984, Val Loss: 0.000302\n",
      "Epoch 9760/20000, Train Loss: 0.003960, Val Loss: 0.000097\n",
      "Epoch 9780/20000, Train Loss: 0.004029, Val Loss: 0.000098\n",
      "Epoch 9800/20000, Train Loss: 0.003886, Val Loss: 0.000086\n",
      "Epoch 9820/20000, Train Loss: 0.004079, Val Loss: 0.000059\n",
      "Epoch 9840/20000, Train Loss: 0.004054, Val Loss: 0.000086\n",
      "Epoch 9860/20000, Train Loss: 0.003946, Val Loss: 0.000185\n",
      "Epoch 9880/20000, Train Loss: 0.004070, Val Loss: 0.000059\n",
      "Epoch 9900/20000, Train Loss: 0.003947, Val Loss: 0.000251\n",
      "Epoch 9920/20000, Train Loss: 0.003940, Val Loss: 0.000096\n",
      "Epoch 9940/20000, Train Loss: 0.003715, Val Loss: 0.000072\n",
      "Epoch 9960/20000, Train Loss: 0.003929, Val Loss: 0.000062\n",
      "Epoch 9980/20000, Train Loss: 0.004033, Val Loss: 0.000101\n",
      "Epoch 10000/20000, Train Loss: 0.003716, Val Loss: 0.000112\n",
      "Epoch 10020/20000, Train Loss: 0.003891, Val Loss: 0.000106\n",
      "Epoch 10040/20000, Train Loss: 0.003816, Val Loss: 0.000080\n",
      "Epoch 10060/20000, Train Loss: 0.004134, Val Loss: 0.000089\n",
      "Epoch 10080/20000, Train Loss: 0.004368, Val Loss: 0.000201\n",
      "Epoch 10100/20000, Train Loss: 0.003831, Val Loss: 0.000150\n",
      "Epoch 10120/20000, Train Loss: 0.003995, Val Loss: 0.000109\n",
      "Epoch 10140/20000, Train Loss: 0.004358, Val Loss: 0.000140\n",
      "Epoch 10160/20000, Train Loss: 0.003905, Val Loss: 0.000092\n",
      "Epoch 10180/20000, Train Loss: 0.003849, Val Loss: 0.000086\n",
      "Epoch 10200/20000, Train Loss: 0.003923, Val Loss: 0.000128\n",
      "Epoch 10220/20000, Train Loss: 0.004064, Val Loss: 0.000070\n",
      "Epoch 10240/20000, Train Loss: 0.003675, Val Loss: 0.000081\n",
      "Epoch 10260/20000, Train Loss: 0.004060, Val Loss: 0.000094\n",
      "Epoch 10280/20000, Train Loss: 0.003711, Val Loss: 0.000116\n",
      "Epoch 10300/20000, Train Loss: 0.003858, Val Loss: 0.000098\n",
      "Epoch 10320/20000, Train Loss: 0.003948, Val Loss: 0.000053\n",
      "Epoch 10340/20000, Train Loss: 0.003922, Val Loss: 0.000092\n",
      "Epoch 10360/20000, Train Loss: 0.004237, Val Loss: 0.000103\n",
      "Epoch 10380/20000, Train Loss: 0.003921, Val Loss: 0.000103\n",
      "Epoch 10400/20000, Train Loss: 0.003701, Val Loss: 0.000218\n",
      "Epoch 10420/20000, Train Loss: 0.004144, Val Loss: 0.000082\n",
      "Epoch 10440/20000, Train Loss: 0.004301, Val Loss: 0.000094\n",
      "Epoch 10460/20000, Train Loss: 0.003826, Val Loss: 0.000080\n",
      "Epoch 10480/20000, Train Loss: 0.003651, Val Loss: 0.000140\n",
      "Epoch 10500/20000, Train Loss: 0.003683, Val Loss: 0.000256\n",
      "Epoch 10520/20000, Train Loss: 0.003848, Val Loss: 0.000084\n",
      "Epoch 10540/20000, Train Loss: 0.003900, Val Loss: 0.000108\n",
      "Epoch 10560/20000, Train Loss: 0.004092, Val Loss: 0.000084\n",
      "Epoch 10580/20000, Train Loss: 0.003933, Val Loss: 0.000087\n",
      "Epoch 10600/20000, Train Loss: 0.003864, Val Loss: 0.000046\n",
      "Epoch 10620/20000, Train Loss: 0.003547, Val Loss: 0.000057\n",
      "Epoch 10640/20000, Train Loss: 0.004280, Val Loss: 0.000159\n",
      "Epoch 10660/20000, Train Loss: 0.003828, Val Loss: 0.000153\n",
      "Epoch 10680/20000, Train Loss: 0.004106, Val Loss: 0.000075\n",
      "Epoch 10700/20000, Train Loss: 0.003735, Val Loss: 0.000108\n",
      "Epoch 10720/20000, Train Loss: 0.004157, Val Loss: 0.000170\n",
      "Epoch 10740/20000, Train Loss: 0.004110, Val Loss: 0.000075\n",
      "Epoch 10760/20000, Train Loss: 0.004040, Val Loss: 0.000072\n",
      "Epoch 10780/20000, Train Loss: 0.003885, Val Loss: 0.000119\n",
      "Epoch 10800/20000, Train Loss: 0.003910, Val Loss: 0.000069\n",
      "Epoch 10820/20000, Train Loss: 0.003861, Val Loss: 0.000077\n",
      "Epoch 10840/20000, Train Loss: 0.004172, Val Loss: 0.000054\n",
      "Epoch 10860/20000, Train Loss: 0.003888, Val Loss: 0.000076\n",
      "Epoch 10880/20000, Train Loss: 0.004203, Val Loss: 0.000057\n",
      "Epoch 10900/20000, Train Loss: 0.003679, Val Loss: 0.000069\n",
      "Epoch 10920/20000, Train Loss: 0.003817, Val Loss: 0.000145\n",
      "Epoch 10940/20000, Train Loss: 0.004001, Val Loss: 0.000122\n",
      "Epoch 10960/20000, Train Loss: 0.003850, Val Loss: 0.000091\n",
      "Epoch 10980/20000, Train Loss: 0.003911, Val Loss: 0.000061\n",
      "Epoch 11000/20000, Train Loss: 0.003729, Val Loss: 0.000070\n",
      "Epoch 11020/20000, Train Loss: 0.003649, Val Loss: 0.000074\n",
      "Epoch 11040/20000, Train Loss: 0.004230, Val Loss: 0.000102\n",
      "Epoch 11060/20000, Train Loss: 0.003985, Val Loss: 0.000040\n",
      "Epoch 11080/20000, Train Loss: 0.003880, Val Loss: 0.000088\n",
      "Epoch 11100/20000, Train Loss: 0.004085, Val Loss: 0.000235\n",
      "Epoch 11120/20000, Train Loss: 0.003868, Val Loss: 0.000121\n",
      "Epoch 11140/20000, Train Loss: 0.003840, Val Loss: 0.000096\n",
      "Epoch 11160/20000, Train Loss: 0.003957, Val Loss: 0.000047\n",
      "Epoch 11180/20000, Train Loss: 0.003866, Val Loss: 0.000060\n",
      "Epoch 11200/20000, Train Loss: 0.003921, Val Loss: 0.000060\n",
      "Epoch 11220/20000, Train Loss: 0.003777, Val Loss: 0.000124\n",
      "Epoch 11240/20000, Train Loss: 0.003970, Val Loss: 0.000075\n",
      "Epoch 11260/20000, Train Loss: 0.003846, Val Loss: 0.000077\n",
      "Epoch 11280/20000, Train Loss: 0.003938, Val Loss: 0.000074\n",
      "Epoch 11300/20000, Train Loss: 0.003915, Val Loss: 0.000073\n",
      "Epoch 11320/20000, Train Loss: 0.003881, Val Loss: 0.000088\n",
      "Epoch 11340/20000, Train Loss: 0.004171, Val Loss: 0.000071\n",
      "Epoch 11360/20000, Train Loss: 0.004072, Val Loss: 0.000159\n",
      "Epoch 11380/20000, Train Loss: 0.003843, Val Loss: 0.000051\n",
      "Epoch 11400/20000, Train Loss: 0.003786, Val Loss: 0.000092\n",
      "Epoch 11420/20000, Train Loss: 0.003660, Val Loss: 0.000120\n",
      "Epoch 11440/20000, Train Loss: 0.003899, Val Loss: 0.000081\n",
      "Epoch 11460/20000, Train Loss: 0.003839, Val Loss: 0.000097\n",
      "Epoch 11480/20000, Train Loss: 0.003946, Val Loss: 0.000124\n",
      "Epoch 11500/20000, Train Loss: 0.003932, Val Loss: 0.000183\n",
      "Epoch 11520/20000, Train Loss: 0.003964, Val Loss: 0.000167\n",
      "Epoch 11540/20000, Train Loss: 0.003669, Val Loss: 0.000136\n",
      "Epoch 11560/20000, Train Loss: 0.003740, Val Loss: 0.000066\n",
      "Epoch 11580/20000, Train Loss: 0.004029, Val Loss: 0.000064\n",
      "Epoch 11600/20000, Train Loss: 0.003773, Val Loss: 0.000078\n",
      "Epoch 11620/20000, Train Loss: 0.004014, Val Loss: 0.000154\n",
      "Epoch 11640/20000, Train Loss: 0.003704, Val Loss: 0.000058\n",
      "Epoch 11660/20000, Train Loss: 0.003968, Val Loss: 0.000159\n",
      "Epoch 11680/20000, Train Loss: 0.003967, Val Loss: 0.000124\n",
      "Epoch 11700/20000, Train Loss: 0.004138, Val Loss: 0.000054\n",
      "Epoch 11720/20000, Train Loss: 0.003762, Val Loss: 0.000155\n",
      "Epoch 11740/20000, Train Loss: 0.004043, Val Loss: 0.000090\n",
      "Epoch 11760/20000, Train Loss: 0.003701, Val Loss: 0.000101\n",
      "Epoch 11780/20000, Train Loss: 0.004119, Val Loss: 0.000073\n",
      "Epoch 11800/20000, Train Loss: 0.003999, Val Loss: 0.000175\n",
      "Epoch 11820/20000, Train Loss: 0.004150, Val Loss: 0.000098\n",
      "Epoch 11840/20000, Train Loss: 0.004065, Val Loss: 0.000070\n",
      "Epoch 11860/20000, Train Loss: 0.003914, Val Loss: 0.000185\n",
      "Epoch 11880/20000, Train Loss: 0.003890, Val Loss: 0.000113\n",
      "Epoch 11900/20000, Train Loss: 0.004038, Val Loss: 0.000094\n",
      "Epoch 11920/20000, Train Loss: 0.004224, Val Loss: 0.000199\n",
      "Epoch 11940/20000, Train Loss: 0.004039, Val Loss: 0.000045\n",
      "Epoch 11960/20000, Train Loss: 0.004010, Val Loss: 0.000072\n",
      "Epoch 11980/20000, Train Loss: 0.004234, Val Loss: 0.000121\n",
      "Epoch 12000/20000, Train Loss: 0.003901, Val Loss: 0.000135\n",
      "Epoch 12020/20000, Train Loss: 0.004125, Val Loss: 0.000089\n",
      "Epoch 12040/20000, Train Loss: 0.003747, Val Loss: 0.000082\n",
      "Epoch 12060/20000, Train Loss: 0.003788, Val Loss: 0.000138\n",
      "Epoch 12080/20000, Train Loss: 0.004107, Val Loss: 0.000122\n",
      "Epoch 12100/20000, Train Loss: 0.003912, Val Loss: 0.000074\n",
      "Epoch 12120/20000, Train Loss: 0.003918, Val Loss: 0.000068\n",
      "Epoch 12140/20000, Train Loss: 0.003748, Val Loss: 0.000125\n",
      "Epoch 12160/20000, Train Loss: 0.003804, Val Loss: 0.000251\n",
      "Epoch 12180/20000, Train Loss: 0.003891, Val Loss: 0.000138\n",
      "Epoch 12200/20000, Train Loss: 0.003798, Val Loss: 0.000090\n",
      "Epoch 12220/20000, Train Loss: 0.004047, Val Loss: 0.000145\n",
      "Epoch 12240/20000, Train Loss: 0.004072, Val Loss: 0.000164\n",
      "Epoch 12260/20000, Train Loss: 0.003806, Val Loss: 0.000132\n",
      "Epoch 12280/20000, Train Loss: 0.003981, Val Loss: 0.000099\n",
      "Epoch 12300/20000, Train Loss: 0.003801, Val Loss: 0.000059\n",
      "Epoch 12320/20000, Train Loss: 0.003882, Val Loss: 0.000062\n",
      "Epoch 12340/20000, Train Loss: 0.003902, Val Loss: 0.000066\n",
      "Epoch 12360/20000, Train Loss: 0.003972, Val Loss: 0.000159\n",
      "Epoch 12380/20000, Train Loss: 0.003913, Val Loss: 0.000099\n",
      "Epoch 12400/20000, Train Loss: 0.004013, Val Loss: 0.000058\n",
      "Epoch 12420/20000, Train Loss: 0.004050, Val Loss: 0.000064\n",
      "Epoch 12440/20000, Train Loss: 0.003931, Val Loss: 0.000101\n",
      "Epoch 12460/20000, Train Loss: 0.003754, Val Loss: 0.000081\n",
      "Epoch 12480/20000, Train Loss: 0.003795, Val Loss: 0.000141\n",
      "Epoch 12500/20000, Train Loss: 0.003747, Val Loss: 0.000075\n",
      "Epoch 12520/20000, Train Loss: 0.003835, Val Loss: 0.000134\n",
      "Epoch 12540/20000, Train Loss: 0.003778, Val Loss: 0.000068\n",
      "Epoch 12560/20000, Train Loss: 0.003783, Val Loss: 0.000194\n",
      "Epoch 12580/20000, Train Loss: 0.003792, Val Loss: 0.000072\n",
      "Epoch 12600/20000, Train Loss: 0.003793, Val Loss: 0.000097\n",
      "Epoch 12620/20000, Train Loss: 0.003933, Val Loss: 0.000084\n",
      "Epoch 12640/20000, Train Loss: 0.003915, Val Loss: 0.000116\n",
      "Epoch 12660/20000, Train Loss: 0.003964, Val Loss: 0.000121\n",
      "Epoch 12680/20000, Train Loss: 0.003911, Val Loss: 0.000091\n",
      "Epoch 12700/20000, Train Loss: 0.003914, Val Loss: 0.000194\n",
      "Epoch 12720/20000, Train Loss: 0.003973, Val Loss: 0.000123\n",
      "Epoch 12740/20000, Train Loss: 0.004071, Val Loss: 0.000064\n",
      "Epoch 12760/20000, Train Loss: 0.003874, Val Loss: 0.000091\n",
      "Epoch 12780/20000, Train Loss: 0.003827, Val Loss: 0.000106\n",
      "Epoch 12800/20000, Train Loss: 0.003684, Val Loss: 0.000102\n",
      "Epoch 12820/20000, Train Loss: 0.003732, Val Loss: 0.000097\n",
      "Epoch 12840/20000, Train Loss: 0.004168, Val Loss: 0.000058\n",
      "Epoch 12860/20000, Train Loss: 0.004148, Val Loss: 0.000115\n",
      "Epoch 12880/20000, Train Loss: 0.003923, Val Loss: 0.000085\n",
      "Epoch 12900/20000, Train Loss: 0.004145, Val Loss: 0.000054\n",
      "Epoch 12920/20000, Train Loss: 0.004148, Val Loss: 0.000086\n",
      "Epoch 12940/20000, Train Loss: 0.003953, Val Loss: 0.000059\n",
      "Epoch 12960/20000, Train Loss: 0.003886, Val Loss: 0.000130\n",
      "Epoch 12980/20000, Train Loss: 0.003979, Val Loss: 0.000099\n",
      "Epoch 13000/20000, Train Loss: 0.004174, Val Loss: 0.000254\n",
      "Epoch 13020/20000, Train Loss: 0.004007, Val Loss: 0.000078\n",
      "Epoch 13040/20000, Train Loss: 0.004108, Val Loss: 0.000058\n",
      "Epoch 13060/20000, Train Loss: 0.003473, Val Loss: 0.000052\n",
      "Epoch 13080/20000, Train Loss: 0.003898, Val Loss: 0.000087\n",
      "Epoch 13100/20000, Train Loss: 0.003877, Val Loss: 0.000095\n",
      "Epoch 13120/20000, Train Loss: 0.004119, Val Loss: 0.000096\n",
      "Epoch 13140/20000, Train Loss: 0.004097, Val Loss: 0.000086\n",
      "Epoch 13160/20000, Train Loss: 0.003875, Val Loss: 0.000083\n",
      "Epoch 13180/20000, Train Loss: 0.003846, Val Loss: 0.000039\n",
      "Epoch 13200/20000, Train Loss: 0.003963, Val Loss: 0.000073\n",
      "Epoch 13220/20000, Train Loss: 0.003778, Val Loss: 0.000076\n",
      "Epoch 13240/20000, Train Loss: 0.003909, Val Loss: 0.000199\n",
      "Epoch 13260/20000, Train Loss: 0.003692, Val Loss: 0.000068\n",
      "Epoch 13280/20000, Train Loss: 0.003894, Val Loss: 0.000091\n",
      "Epoch 13300/20000, Train Loss: 0.003779, Val Loss: 0.000126\n",
      "Epoch 13320/20000, Train Loss: 0.003897, Val Loss: 0.000120\n",
      "Epoch 13340/20000, Train Loss: 0.004157, Val Loss: 0.000141\n",
      "Epoch 13360/20000, Train Loss: 0.003834, Val Loss: 0.000065\n",
      "Epoch 13380/20000, Train Loss: 0.004011, Val Loss: 0.000090\n",
      "Epoch 13400/20000, Train Loss: 0.003873, Val Loss: 0.000133\n",
      "Epoch 13420/20000, Train Loss: 0.004038, Val Loss: 0.000064\n",
      "Epoch 13440/20000, Train Loss: 0.003633, Val Loss: 0.000118\n",
      "Epoch 13460/20000, Train Loss: 0.003882, Val Loss: 0.000068\n",
      "Epoch 13480/20000, Train Loss: 0.004271, Val Loss: 0.000134\n",
      "Epoch 13500/20000, Train Loss: 0.003962, Val Loss: 0.000092\n",
      "Epoch 13520/20000, Train Loss: 0.003913, Val Loss: 0.000139\n",
      "Epoch 13540/20000, Train Loss: 0.003833, Val Loss: 0.000185\n",
      "Epoch 13560/20000, Train Loss: 0.003926, Val Loss: 0.000122\n",
      "Epoch 13580/20000, Train Loss: 0.003952, Val Loss: 0.000216\n",
      "Epoch 13600/20000, Train Loss: 0.003570, Val Loss: 0.000065\n",
      "Epoch 13620/20000, Train Loss: 0.003717, Val Loss: 0.000102\n",
      "Epoch 13640/20000, Train Loss: 0.004016, Val Loss: 0.000085\n",
      "Epoch 13660/20000, Train Loss: 0.003736, Val Loss: 0.000047\n",
      "Epoch 13680/20000, Train Loss: 0.003564, Val Loss: 0.000047\n",
      "Epoch 13700/20000, Train Loss: 0.003873, Val Loss: 0.000225\n",
      "Epoch 13720/20000, Train Loss: 0.003764, Val Loss: 0.000059\n",
      "Epoch 13740/20000, Train Loss: 0.003925, Val Loss: 0.000084\n",
      "Epoch 13760/20000, Train Loss: 0.003978, Val Loss: 0.000106\n",
      "Epoch 13780/20000, Train Loss: 0.003913, Val Loss: 0.000091\n",
      "Epoch 13800/20000, Train Loss: 0.004011, Val Loss: 0.000072\n",
      "Epoch 13820/20000, Train Loss: 0.004142, Val Loss: 0.000068\n",
      "Epoch 13840/20000, Train Loss: 0.003633, Val Loss: 0.000100\n",
      "Epoch 13860/20000, Train Loss: 0.004053, Val Loss: 0.000063\n",
      "Epoch 13880/20000, Train Loss: 0.004084, Val Loss: 0.000090\n",
      "Epoch 13900/20000, Train Loss: 0.003891, Val Loss: 0.000052\n",
      "Epoch 13920/20000, Train Loss: 0.003766, Val Loss: 0.000106\n",
      "Epoch 13940/20000, Train Loss: 0.003945, Val Loss: 0.000052\n",
      "Epoch 13960/20000, Train Loss: 0.003937, Val Loss: 0.000169\n",
      "Epoch 13980/20000, Train Loss: 0.003996, Val Loss: 0.000213\n",
      "Epoch 14000/20000, Train Loss: 0.003875, Val Loss: 0.000069\n",
      "Epoch 14020/20000, Train Loss: 0.003684, Val Loss: 0.000062\n",
      "Epoch 14040/20000, Train Loss: 0.003925, Val Loss: 0.000086\n",
      "Epoch 14060/20000, Train Loss: 0.003754, Val Loss: 0.000069\n",
      "Epoch 14080/20000, Train Loss: 0.003862, Val Loss: 0.000103\n",
      "Epoch 14100/20000, Train Loss: 0.003944, Val Loss: 0.000165\n",
      "Epoch 14120/20000, Train Loss: 0.003947, Val Loss: 0.000098\n",
      "Epoch 14140/20000, Train Loss: 0.003873, Val Loss: 0.000109\n",
      "Epoch 14160/20000, Train Loss: 0.003852, Val Loss: 0.000113\n",
      "Epoch 14180/20000, Train Loss: 0.003985, Val Loss: 0.000101\n",
      "Epoch 14200/20000, Train Loss: 0.003761, Val Loss: 0.000084\n",
      "Epoch 14220/20000, Train Loss: 0.003783, Val Loss: 0.000104\n",
      "Epoch 14240/20000, Train Loss: 0.003888, Val Loss: 0.000117\n",
      "Epoch 14260/20000, Train Loss: 0.003840, Val Loss: 0.000059\n",
      "Epoch 14280/20000, Train Loss: 0.003882, Val Loss: 0.000058\n",
      "Epoch 14300/20000, Train Loss: 0.003847, Val Loss: 0.000134\n",
      "Epoch 14320/20000, Train Loss: 0.003800, Val Loss: 0.000123\n",
      "Epoch 14340/20000, Train Loss: 0.004042, Val Loss: 0.000104\n",
      "Epoch 14360/20000, Train Loss: 0.003889, Val Loss: 0.000079\n",
      "Epoch 14380/20000, Train Loss: 0.003731, Val Loss: 0.000060\n",
      "Epoch 14400/20000, Train Loss: 0.003705, Val Loss: 0.000145\n",
      "Epoch 14420/20000, Train Loss: 0.003772, Val Loss: 0.000073\n",
      "Epoch 14440/20000, Train Loss: 0.003889, Val Loss: 0.000083\n",
      "Epoch 14460/20000, Train Loss: 0.003779, Val Loss: 0.000077\n",
      "Epoch 14480/20000, Train Loss: 0.003933, Val Loss: 0.000086\n",
      "Epoch 14500/20000, Train Loss: 0.003734, Val Loss: 0.000045\n",
      "Epoch 14520/20000, Train Loss: 0.003829, Val Loss: 0.000086\n",
      "Epoch 14540/20000, Train Loss: 0.003817, Val Loss: 0.000170\n",
      "Epoch 14560/20000, Train Loss: 0.003945, Val Loss: 0.000074\n",
      "Epoch 14580/20000, Train Loss: 0.003831, Val Loss: 0.000054\n",
      "Epoch 14600/20000, Train Loss: 0.004060, Val Loss: 0.000087\n",
      "Epoch 14620/20000, Train Loss: 0.003884, Val Loss: 0.000110\n",
      "Epoch 14640/20000, Train Loss: 0.003685, Val Loss: 0.000081\n",
      "Epoch 14660/20000, Train Loss: 0.003722, Val Loss: 0.000082\n",
      "Epoch 14680/20000, Train Loss: 0.003952, Val Loss: 0.000082\n",
      "Epoch 14700/20000, Train Loss: 0.003821, Val Loss: 0.000163\n",
      "Epoch 14720/20000, Train Loss: 0.003947, Val Loss: 0.000118\n",
      "Epoch 14740/20000, Train Loss: 0.003855, Val Loss: 0.000079\n",
      "Epoch 14760/20000, Train Loss: 0.004015, Val Loss: 0.000055\n",
      "Epoch 14780/20000, Train Loss: 0.003886, Val Loss: 0.000136\n",
      "Epoch 14800/20000, Train Loss: 0.003965, Val Loss: 0.000093\n",
      "Epoch 14820/20000, Train Loss: 0.003934, Val Loss: 0.000069\n",
      "Epoch 14840/20000, Train Loss: 0.003819, Val Loss: 0.000167\n",
      "Epoch 14860/20000, Train Loss: 0.003988, Val Loss: 0.000108\n",
      "Epoch 14880/20000, Train Loss: 0.003818, Val Loss: 0.000122\n",
      "Epoch 14900/20000, Train Loss: 0.003697, Val Loss: 0.000087\n",
      "Epoch 14920/20000, Train Loss: 0.003755, Val Loss: 0.000131\n",
      "Epoch 14940/20000, Train Loss: 0.003651, Val Loss: 0.000104\n",
      "Epoch 14960/20000, Train Loss: 0.003911, Val Loss: 0.000034\n",
      "Epoch 14980/20000, Train Loss: 0.004030, Val Loss: 0.000061\n",
      "Epoch 15000/20000, Train Loss: 0.003882, Val Loss: 0.000087\n",
      "Epoch 15020/20000, Train Loss: 0.004065, Val Loss: 0.000134\n",
      "Epoch 15040/20000, Train Loss: 0.004033, Val Loss: 0.000080\n",
      "Epoch 15060/20000, Train Loss: 0.003813, Val Loss: 0.000102\n",
      "Epoch 15080/20000, Train Loss: 0.003862, Val Loss: 0.000069\n",
      "Epoch 15100/20000, Train Loss: 0.003635, Val Loss: 0.000081\n",
      "Epoch 15120/20000, Train Loss: 0.003877, Val Loss: 0.000094\n",
      "Epoch 15140/20000, Train Loss: 0.003980, Val Loss: 0.000100\n",
      "Epoch 15160/20000, Train Loss: 0.003994, Val Loss: 0.000108\n",
      "Epoch 15180/20000, Train Loss: 0.003808, Val Loss: 0.000065\n",
      "Epoch 15200/20000, Train Loss: 0.003821, Val Loss: 0.000097\n",
      "Epoch 15220/20000, Train Loss: 0.003908, Val Loss: 0.000039\n",
      "Epoch 15240/20000, Train Loss: 0.003847, Val Loss: 0.000085\n",
      "Epoch 15260/20000, Train Loss: 0.003722, Val Loss: 0.000048\n",
      "Epoch 15280/20000, Train Loss: 0.003705, Val Loss: 0.000075\n",
      "Epoch 15300/20000, Train Loss: 0.003950, Val Loss: 0.000071\n",
      "Epoch 15320/20000, Train Loss: 0.003785, Val Loss: 0.000093\n",
      "Epoch 15340/20000, Train Loss: 0.003690, Val Loss: 0.000180\n",
      "Epoch 15360/20000, Train Loss: 0.003744, Val Loss: 0.000050\n",
      "Epoch 15380/20000, Train Loss: 0.003830, Val Loss: 0.000071\n",
      "Epoch 15400/20000, Train Loss: 0.003965, Val Loss: 0.000141\n",
      "Epoch 15420/20000, Train Loss: 0.003868, Val Loss: 0.000101\n",
      "Epoch 15440/20000, Train Loss: 0.003822, Val Loss: 0.000056\n",
      "Epoch 15460/20000, Train Loss: 0.004057, Val Loss: 0.000118\n",
      "Epoch 15480/20000, Train Loss: 0.003911, Val Loss: 0.000045\n",
      "Epoch 15500/20000, Train Loss: 0.003653, Val Loss: 0.000051\n",
      "Epoch 15520/20000, Train Loss: 0.003736, Val Loss: 0.000076\n",
      "Epoch 15540/20000, Train Loss: 0.003820, Val Loss: 0.000079\n",
      "Epoch 15560/20000, Train Loss: 0.003970, Val Loss: 0.000043\n",
      "Epoch 15580/20000, Train Loss: 0.003972, Val Loss: 0.000130\n",
      "Epoch 15600/20000, Train Loss: 0.003732, Val Loss: 0.000093\n",
      "Epoch 15620/20000, Train Loss: 0.003927, Val Loss: 0.000063\n",
      "Epoch 15640/20000, Train Loss: 0.003616, Val Loss: 0.000103\n",
      "Epoch 15660/20000, Train Loss: 0.003724, Val Loss: 0.000113\n",
      "Epoch 15680/20000, Train Loss: 0.003774, Val Loss: 0.000097\n",
      "Epoch 15700/20000, Train Loss: 0.004022, Val Loss: 0.000106\n",
      "Epoch 15720/20000, Train Loss: 0.003863, Val Loss: 0.000074\n",
      "Epoch 15740/20000, Train Loss: 0.003822, Val Loss: 0.000122\n",
      "Epoch 15760/20000, Train Loss: 0.003757, Val Loss: 0.000099\n",
      "Epoch 15780/20000, Train Loss: 0.003810, Val Loss: 0.000055\n",
      "Epoch 15800/20000, Train Loss: 0.003838, Val Loss: 0.000130\n",
      "Epoch 15820/20000, Train Loss: 0.003636, Val Loss: 0.000086\n",
      "Epoch 15840/20000, Train Loss: 0.003666, Val Loss: 0.000070\n",
      "Epoch 15860/20000, Train Loss: 0.003970, Val Loss: 0.000096\n",
      "Epoch 15880/20000, Train Loss: 0.004066, Val Loss: 0.000112\n",
      "Epoch 15900/20000, Train Loss: 0.003986, Val Loss: 0.000092\n",
      "Epoch 15920/20000, Train Loss: 0.003862, Val Loss: 0.000131\n",
      "Epoch 15940/20000, Train Loss: 0.004003, Val Loss: 0.000045\n",
      "Epoch 15960/20000, Train Loss: 0.003833, Val Loss: 0.000066\n",
      "Epoch 15980/20000, Train Loss: 0.003658, Val Loss: 0.000064\n",
      "Epoch 16000/20000, Train Loss: 0.003674, Val Loss: 0.000066\n",
      "Epoch 16020/20000, Train Loss: 0.004054, Val Loss: 0.000041\n",
      "Epoch 16040/20000, Train Loss: 0.003952, Val Loss: 0.000057\n",
      "Epoch 16060/20000, Train Loss: 0.003909, Val Loss: 0.000126\n",
      "Epoch 16080/20000, Train Loss: 0.003939, Val Loss: 0.000070\n",
      "Epoch 16100/20000, Train Loss: 0.003726, Val Loss: 0.000065\n",
      "Epoch 16120/20000, Train Loss: 0.003871, Val Loss: 0.000080\n",
      "Epoch 16140/20000, Train Loss: 0.003752, Val Loss: 0.000069\n",
      "Epoch 16160/20000, Train Loss: 0.004034, Val Loss: 0.000110\n",
      "Epoch 16180/20000, Train Loss: 0.003853, Val Loss: 0.000060\n",
      "Epoch 16200/20000, Train Loss: 0.004000, Val Loss: 0.000077\n",
      "Epoch 16220/20000, Train Loss: 0.003773, Val Loss: 0.000105\n",
      "Epoch 16240/20000, Train Loss: 0.003835, Val Loss: 0.000113\n",
      "Epoch 16260/20000, Train Loss: 0.004033, Val Loss: 0.000130\n",
      "Epoch 16280/20000, Train Loss: 0.003910, Val Loss: 0.000062\n",
      "Epoch 16300/20000, Train Loss: 0.003775, Val Loss: 0.000097\n",
      "Epoch 16320/20000, Train Loss: 0.003907, Val Loss: 0.000139\n",
      "Epoch 16340/20000, Train Loss: 0.003840, Val Loss: 0.000062\n",
      "Epoch 16360/20000, Train Loss: 0.003870, Val Loss: 0.000134\n",
      "Epoch 16380/20000, Train Loss: 0.003883, Val Loss: 0.000068\n",
      "Epoch 16400/20000, Train Loss: 0.003955, Val Loss: 0.000071\n",
      "Epoch 16420/20000, Train Loss: 0.003858, Val Loss: 0.000102\n",
      "Epoch 16440/20000, Train Loss: 0.004043, Val Loss: 0.000131\n",
      "Epoch 16460/20000, Train Loss: 0.003806, Val Loss: 0.000065\n",
      "Epoch 16480/20000, Train Loss: 0.004064, Val Loss: 0.000076\n",
      "Epoch 16500/20000, Train Loss: 0.003726, Val Loss: 0.000070\n",
      "Epoch 16520/20000, Train Loss: 0.003736, Val Loss: 0.000072\n",
      "Epoch 16540/20000, Train Loss: 0.003818, Val Loss: 0.000090\n",
      "Epoch 16560/20000, Train Loss: 0.003843, Val Loss: 0.000058\n",
      "Epoch 16580/20000, Train Loss: 0.003715, Val Loss: 0.000058\n",
      "Epoch 16600/20000, Train Loss: 0.003958, Val Loss: 0.000087\n",
      "Epoch 16620/20000, Train Loss: 0.004104, Val Loss: 0.000037\n",
      "Epoch 16640/20000, Train Loss: 0.003955, Val Loss: 0.000060\n",
      "Epoch 16660/20000, Train Loss: 0.004016, Val Loss: 0.000112\n",
      "Epoch 16680/20000, Train Loss: 0.003759, Val Loss: 0.000087\n",
      "Epoch 16700/20000, Train Loss: 0.003903, Val Loss: 0.000093\n",
      "Epoch 16720/20000, Train Loss: 0.003988, Val Loss: 0.000097\n",
      "Epoch 16740/20000, Train Loss: 0.003756, Val Loss: 0.000062\n",
      "Epoch 16760/20000, Train Loss: 0.003783, Val Loss: 0.000072\n",
      "Epoch 16780/20000, Train Loss: 0.003989, Val Loss: 0.000087\n",
      "Epoch 16800/20000, Train Loss: 0.003705, Val Loss: 0.000103\n",
      "Epoch 16820/20000, Train Loss: 0.003862, Val Loss: 0.000073\n",
      "Epoch 16840/20000, Train Loss: 0.004039, Val Loss: 0.000113\n",
      "Epoch 16860/20000, Train Loss: 0.003774, Val Loss: 0.000131\n",
      "Epoch 16880/20000, Train Loss: 0.004008, Val Loss: 0.000107\n",
      "Epoch 16900/20000, Train Loss: 0.003965, Val Loss: 0.000103\n",
      "Epoch 16920/20000, Train Loss: 0.003882, Val Loss: 0.000078\n",
      "Epoch 16940/20000, Train Loss: 0.003932, Val Loss: 0.000077\n",
      "Epoch 16960/20000, Train Loss: 0.003805, Val Loss: 0.000063\n",
      "Epoch 16980/20000, Train Loss: 0.004173, Val Loss: 0.000123\n",
      "Epoch 17000/20000, Train Loss: 0.003966, Val Loss: 0.000111\n",
      "Epoch 17020/20000, Train Loss: 0.003899, Val Loss: 0.000064\n",
      "Epoch 17040/20000, Train Loss: 0.003707, Val Loss: 0.000086\n",
      "Epoch 17060/20000, Train Loss: 0.003884, Val Loss: 0.000063\n",
      "Epoch 17080/20000, Train Loss: 0.003933, Val Loss: 0.000116\n",
      "Epoch 17100/20000, Train Loss: 0.004147, Val Loss: 0.000110\n",
      "Epoch 17120/20000, Train Loss: 0.004051, Val Loss: 0.000040\n",
      "Epoch 17140/20000, Train Loss: 0.003856, Val Loss: 0.000059\n",
      "Epoch 17160/20000, Train Loss: 0.004134, Val Loss: 0.000149\n",
      "Epoch 17180/20000, Train Loss: 0.003935, Val Loss: 0.000056\n",
      "Epoch 17200/20000, Train Loss: 0.004005, Val Loss: 0.000076\n",
      "Epoch 17220/20000, Train Loss: 0.003766, Val Loss: 0.000091\n",
      "Epoch 17240/20000, Train Loss: 0.003987, Val Loss: 0.000106\n",
      "Epoch 17260/20000, Train Loss: 0.003812, Val Loss: 0.000069\n",
      "Epoch 17280/20000, Train Loss: 0.003861, Val Loss: 0.000082\n",
      "Epoch 17300/20000, Train Loss: 0.003962, Val Loss: 0.000066\n",
      "Epoch 17320/20000, Train Loss: 0.003723, Val Loss: 0.000171\n",
      "Epoch 17340/20000, Train Loss: 0.003845, Val Loss: 0.000099\n",
      "Epoch 17360/20000, Train Loss: 0.003704, Val Loss: 0.000070\n",
      "Epoch 17380/20000, Train Loss: 0.003930, Val Loss: 0.000060\n",
      "Epoch 17400/20000, Train Loss: 0.003705, Val Loss: 0.000061\n",
      "Epoch 17420/20000, Train Loss: 0.003885, Val Loss: 0.000057\n",
      "Epoch 17440/20000, Train Loss: 0.004122, Val Loss: 0.000052\n",
      "Epoch 17460/20000, Train Loss: 0.003915, Val Loss: 0.000105\n",
      "Epoch 17480/20000, Train Loss: 0.004057, Val Loss: 0.000049\n",
      "Epoch 17500/20000, Train Loss: 0.003610, Val Loss: 0.000058\n",
      "Epoch 17520/20000, Train Loss: 0.003787, Val Loss: 0.000072\n",
      "Epoch 17540/20000, Train Loss: 0.003613, Val Loss: 0.000057\n",
      "Epoch 17560/20000, Train Loss: 0.004055, Val Loss: 0.000141\n",
      "Epoch 17580/20000, Train Loss: 0.003733, Val Loss: 0.000081\n",
      "Epoch 17600/20000, Train Loss: 0.003974, Val Loss: 0.000053\n",
      "Epoch 17620/20000, Train Loss: 0.003775, Val Loss: 0.000053\n",
      "Epoch 17640/20000, Train Loss: 0.003792, Val Loss: 0.000050\n",
      "Epoch 17660/20000, Train Loss: 0.003936, Val Loss: 0.000116\n",
      "Epoch 17680/20000, Train Loss: 0.003724, Val Loss: 0.000110\n",
      "Epoch 17700/20000, Train Loss: 0.003970, Val Loss: 0.000168\n",
      "Epoch 17720/20000, Train Loss: 0.003604, Val Loss: 0.000057\n",
      "Epoch 17740/20000, Train Loss: 0.003732, Val Loss: 0.000081\n",
      "Epoch 17760/20000, Train Loss: 0.003938, Val Loss: 0.000072\n",
      "Epoch 17780/20000, Train Loss: 0.003963, Val Loss: 0.000051\n",
      "Epoch 17800/20000, Train Loss: 0.003988, Val Loss: 0.000125\n",
      "Epoch 17820/20000, Train Loss: 0.004017, Val Loss: 0.000077\n",
      "Epoch 17840/20000, Train Loss: 0.004102, Val Loss: 0.000054\n",
      "Epoch 17860/20000, Train Loss: 0.003861, Val Loss: 0.000077\n",
      "Epoch 17880/20000, Train Loss: 0.003584, Val Loss: 0.000061\n",
      "Epoch 17900/20000, Train Loss: 0.003915, Val Loss: 0.000089\n",
      "Epoch 17920/20000, Train Loss: 0.003790, Val Loss: 0.000101\n",
      "Epoch 17940/20000, Train Loss: 0.003690, Val Loss: 0.000063\n",
      "Epoch 17960/20000, Train Loss: 0.003826, Val Loss: 0.000082\n",
      "Epoch 17980/20000, Train Loss: 0.003976, Val Loss: 0.000099\n",
      "Epoch 18000/20000, Train Loss: 0.003747, Val Loss: 0.000066\n",
      "Epoch 18020/20000, Train Loss: 0.003883, Val Loss: 0.000070\n",
      "Epoch 18040/20000, Train Loss: 0.003885, Val Loss: 0.000088\n",
      "Epoch 18060/20000, Train Loss: 0.003642, Val Loss: 0.000053\n",
      "Epoch 18080/20000, Train Loss: 0.003941, Val Loss: 0.000049\n",
      "Epoch 18100/20000, Train Loss: 0.003674, Val Loss: 0.000070\n",
      "Epoch 18120/20000, Train Loss: 0.003908, Val Loss: 0.000088\n",
      "Epoch 18140/20000, Train Loss: 0.003918, Val Loss: 0.000048\n",
      "Epoch 18160/20000, Train Loss: 0.003871, Val Loss: 0.000069\n",
      "Epoch 18180/20000, Train Loss: 0.003998, Val Loss: 0.000169\n",
      "Epoch 18200/20000, Train Loss: 0.003730, Val Loss: 0.000052\n",
      "Epoch 18220/20000, Train Loss: 0.003886, Val Loss: 0.000037\n",
      "Epoch 18240/20000, Train Loss: 0.003835, Val Loss: 0.000107\n",
      "Epoch 18260/20000, Train Loss: 0.003968, Val Loss: 0.000079\n",
      "Epoch 18280/20000, Train Loss: 0.003799, Val Loss: 0.000103\n",
      "Epoch 18300/20000, Train Loss: 0.003796, Val Loss: 0.000078\n",
      "Epoch 18320/20000, Train Loss: 0.003887, Val Loss: 0.000043\n",
      "Epoch 18340/20000, Train Loss: 0.003978, Val Loss: 0.000064\n",
      "Epoch 18360/20000, Train Loss: 0.003800, Val Loss: 0.000082\n",
      "Epoch 18380/20000, Train Loss: 0.003536, Val Loss: 0.000053\n",
      "Epoch 18400/20000, Train Loss: 0.003685, Val Loss: 0.000093\n",
      "Epoch 18420/20000, Train Loss: 0.003979, Val Loss: 0.000063\n",
      "Epoch 18440/20000, Train Loss: 0.003747, Val Loss: 0.000143\n",
      "Epoch 18460/20000, Train Loss: 0.003894, Val Loss: 0.000082\n",
      "Epoch 18480/20000, Train Loss: 0.003889, Val Loss: 0.000061\n",
      "Epoch 18500/20000, Train Loss: 0.003777, Val Loss: 0.000066\n",
      "Epoch 18520/20000, Train Loss: 0.003814, Val Loss: 0.000058\n",
      "Epoch 18540/20000, Train Loss: 0.003815, Val Loss: 0.000083\n",
      "Epoch 18560/20000, Train Loss: 0.003605, Val Loss: 0.000060\n",
      "Epoch 18580/20000, Train Loss: 0.003901, Val Loss: 0.000084\n",
      "Epoch 18600/20000, Train Loss: 0.003802, Val Loss: 0.000047\n",
      "Epoch 18620/20000, Train Loss: 0.003742, Val Loss: 0.000084\n",
      "Epoch 18640/20000, Train Loss: 0.003896, Val Loss: 0.000176\n",
      "Epoch 18660/20000, Train Loss: 0.003774, Val Loss: 0.000081\n",
      "Epoch 18680/20000, Train Loss: 0.003538, Val Loss: 0.000085\n",
      "Epoch 18700/20000, Train Loss: 0.003654, Val Loss: 0.000060\n",
      "Epoch 18720/20000, Train Loss: 0.004031, Val Loss: 0.000103\n",
      "Epoch 18740/20000, Train Loss: 0.003759, Val Loss: 0.000054\n",
      "Epoch 18760/20000, Train Loss: 0.003986, Val Loss: 0.000124\n",
      "Epoch 18780/20000, Train Loss: 0.004078, Val Loss: 0.000194\n",
      "Epoch 18800/20000, Train Loss: 0.003715, Val Loss: 0.000049\n",
      "Epoch 18820/20000, Train Loss: 0.003968, Val Loss: 0.000058\n",
      "Epoch 18840/20000, Train Loss: 0.003781, Val Loss: 0.000082\n",
      "Epoch 18860/20000, Train Loss: 0.003643, Val Loss: 0.000073\n",
      "Epoch 18880/20000, Train Loss: 0.003899, Val Loss: 0.000091\n",
      "Epoch 18900/20000, Train Loss: 0.004076, Val Loss: 0.000078\n",
      "Epoch 18920/20000, Train Loss: 0.004024, Val Loss: 0.000098\n",
      "Epoch 18940/20000, Train Loss: 0.003925, Val Loss: 0.000078\n",
      "Epoch 18960/20000, Train Loss: 0.003929, Val Loss: 0.000049\n",
      "Epoch 18980/20000, Train Loss: 0.003815, Val Loss: 0.000047\n",
      "Epoch 19000/20000, Train Loss: 0.003826, Val Loss: 0.000077\n",
      "Epoch 19020/20000, Train Loss: 0.003796, Val Loss: 0.000062\n",
      "Epoch 19040/20000, Train Loss: 0.003735, Val Loss: 0.000040\n",
      "Epoch 19060/20000, Train Loss: 0.003705, Val Loss: 0.000048\n",
      "Epoch 19080/20000, Train Loss: 0.003649, Val Loss: 0.000079\n",
      "Epoch 19100/20000, Train Loss: 0.003520, Val Loss: 0.000103\n",
      "Epoch 19120/20000, Train Loss: 0.003829, Val Loss: 0.000052\n",
      "Epoch 19140/20000, Train Loss: 0.003676, Val Loss: 0.000099\n",
      "Epoch 19160/20000, Train Loss: 0.003860, Val Loss: 0.000082\n",
      "Epoch 19180/20000, Train Loss: 0.003814, Val Loss: 0.000091\n",
      "Epoch 19200/20000, Train Loss: 0.003991, Val Loss: 0.000067\n",
      "Epoch 19220/20000, Train Loss: 0.003812, Val Loss: 0.000050\n",
      "Epoch 19240/20000, Train Loss: 0.003746, Val Loss: 0.000064\n",
      "Epoch 19260/20000, Train Loss: 0.003839, Val Loss: 0.000213\n",
      "Epoch 19280/20000, Train Loss: 0.003696, Val Loss: 0.000063\n",
      "Epoch 19300/20000, Train Loss: 0.003875, Val Loss: 0.000105\n",
      "Epoch 19320/20000, Train Loss: 0.003683, Val Loss: 0.000059\n",
      "Epoch 19340/20000, Train Loss: 0.003744, Val Loss: 0.000049\n",
      "Epoch 19360/20000, Train Loss: 0.003891, Val Loss: 0.000181\n",
      "Epoch 19380/20000, Train Loss: 0.003899, Val Loss: 0.000079\n",
      "Epoch 19400/20000, Train Loss: 0.003910, Val Loss: 0.000059\n",
      "Epoch 19420/20000, Train Loss: 0.003890, Val Loss: 0.000142\n",
      "Epoch 19440/20000, Train Loss: 0.003945, Val Loss: 0.000034\n",
      "Epoch 19460/20000, Train Loss: 0.003956, Val Loss: 0.000045\n",
      "Epoch 19480/20000, Train Loss: 0.003716, Val Loss: 0.000063\n",
      "Epoch 19500/20000, Train Loss: 0.003735, Val Loss: 0.000065\n",
      "Epoch 19520/20000, Train Loss: 0.003651, Val Loss: 0.000114\n",
      "Epoch 19540/20000, Train Loss: 0.003694, Val Loss: 0.000141\n",
      "Epoch 19560/20000, Train Loss: 0.003989, Val Loss: 0.000088\n",
      "Epoch 19580/20000, Train Loss: 0.004045, Val Loss: 0.000171\n",
      "Epoch 19600/20000, Train Loss: 0.003871, Val Loss: 0.000106\n",
      "Epoch 19620/20000, Train Loss: 0.003717, Val Loss: 0.000053\n",
      "Epoch 19640/20000, Train Loss: 0.003790, Val Loss: 0.000078\n",
      "Epoch 19660/20000, Train Loss: 0.003992, Val Loss: 0.000048\n",
      "Epoch 19680/20000, Train Loss: 0.003730, Val Loss: 0.000113\n",
      "Epoch 19700/20000, Train Loss: 0.003831, Val Loss: 0.000059\n",
      "Epoch 19720/20000, Train Loss: 0.003731, Val Loss: 0.000131\n",
      "Epoch 19740/20000, Train Loss: 0.003982, Val Loss: 0.000090\n",
      "Epoch 19760/20000, Train Loss: 0.003721, Val Loss: 0.000059\n",
      "Epoch 19780/20000, Train Loss: 0.003773, Val Loss: 0.000147\n",
      "Epoch 19800/20000, Train Loss: 0.003535, Val Loss: 0.000064\n",
      "Epoch 19820/20000, Train Loss: 0.003617, Val Loss: 0.000074\n",
      "Epoch 19840/20000, Train Loss: 0.003937, Val Loss: 0.000067\n",
      "Epoch 19860/20000, Train Loss: 0.003953, Val Loss: 0.000033\n",
      "Epoch 19880/20000, Train Loss: 0.003690, Val Loss: 0.000070\n",
      "Epoch 19900/20000, Train Loss: 0.003889, Val Loss: 0.000128\n",
      "Epoch 19920/20000, Train Loss: 0.003776, Val Loss: 0.000058\n",
      "Epoch 19940/20000, Train Loss: 0.003918, Val Loss: 0.000051\n",
      "Epoch 19960/20000, Train Loss: 0.003880, Val Loss: 0.000062\n",
      "Epoch 19980/20000, Train Loss: 0.003911, Val Loss: 0.000047\n",
      "Epoch 20000/20000, Train Loss: 0.003832, Val Loss: 0.000208\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UREA1: 1.18 units  0.8%\n",
      "UREA2: 1.81 units  0.4%\n",
      "UREA3: 1.81 units  0.4%\n",
      "DAP: 1.64 units  0.8%\n",
      "MOP: 1.67 units  0.6%\n",
      "organic: 6.00 units  0.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# 1 Prepare dataset\n",
    "# =========================\n",
    "soil_cols = [\n",
    "    'ph','organic_matter','total_nitrogen','potassium','p2o5','boron','zinc',\n",
    "    'sand','clay','slit','parentsoil','crop','variety'\n",
    "]\n",
    "fert_cols = ['UREA1','UREA2','UREA3','DAP','MOP','organic']\n",
    "\n",
    "# Load df as your dataset\n",
    "df = df.dropna(subset=soil_cols + fert_cols)\n",
    "df = df.sample(min(25000, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = df[soil_cols]\n",
    "y = df[fert_cols]\n",
    "\n",
    "# Feature & target scaling\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Precompute min/max for clipping\n",
    "fert_min = np.zeros(len(fert_cols))           # no negative fertilizers\n",
    "fert_max = y.max().values                     # realistic max per fertilizer\n",
    "\n",
    "# =========================\n",
    "# 2 Define model with Dropout\n",
    "# =========================\n",
    "class FertNetDropout(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FertNetDropout(X_train.shape[1], y_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# =========================\n",
    "# 3 Train the model\n",
    "# =========================\n",
    "epochs = 20000\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_list.append(loss.item())\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test)\n",
    "        val_loss = criterion(val_outputs, y_test)\n",
    "        val_loss_list.append(val_loss.item())\n",
    "\n",
    "    if ep % 20 == 0 or ep == 1:\n",
    "        print(f\"Epoch {ep}/{epochs}, Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"fertilizer_model_dropout.pth\")\n",
    "\n",
    "# =========================\n",
    "# 4 Plot Loss\n",
    "# =========================\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 5 Prediction with % uncertainty\n",
    "# =========================\n",
    "def predict_fertilizer_with_percent_uncertainty(input_features, mc_runs=50):\n",
    "    \"\"\"\n",
    "    Predict fertilizer amounts with Monte Carlo Dropout uncertainty (as %)\n",
    "    input_features: list or array of soil features in the order of soil_cols\n",
    "    mc_runs: number of forward passes for uncertainty estimation\n",
    "    Returns: mean prediction and percentage uncertainty for each fertilizer\n",
    "    \"\"\"\n",
    "    # Convert input to DataFrame to avoid sklearn warning\n",
    "    x_df = pd.DataFrame([input_features], columns=soil_cols)\n",
    "    x_scaled = scaler_X.transform(x_df)\n",
    "    x_tensor = torch.tensor(x_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Enable dropout during inference\n",
    "    model.train()\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(mc_runs):\n",
    "        with torch.no_grad():\n",
    "            y_scaled_pred = model(x_tensor).numpy()\n",
    "        y_pred = scaler_y.inverse_transform(y_scaled_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, fert_min, fert_max)\n",
    "        preds.append(y_pred_clipped.flatten())\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    pred_mean = preds.mean(axis=0)\n",
    "    pred_std = preds.std(axis=0)\n",
    "    \n",
    "    # Convert std to percentage of predicted amount\n",
    "    pred_percent_uncertainty = (pred_std / pred_mean) * 100\n",
    "    return pred_mean, pred_percent_uncertainty\n",
    "\n",
    "# =========================\n",
    "# 6 Example usage\n",
    "# =========================\n",
    "sample_input = X.iloc[0].values\n",
    "pred_mean, pred_percent = predict_fertilizer_with_percent_uncertainty(sample_input, mc_runs=50)\n",
    "\n",
    "for f, m, p in zip(fert_cols, pred_mean, pred_percent):\n",
    "    print(f\"{f}: {m:.2f} units  {p:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d3b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UREA1: 0.97 units  2.9%\n",
      "UREA2: 1.81 units  0.9%\n",
      "UREA3: 1.81 units  0.9%\n",
      "DAP: 2.17 units  2.0%\n",
      "MOP: 1.67 units  0.9%\n",
      "organic: 6.00 units  0.0%\n"
     ]
    }
   ],
   "source": [
    "# 1 Extract soil features manually from your CSV row\n",
    "manual_input = [\n",
    "    5.69,   # ph\n",
    "    4.29,   # organic_matter\n",
    "    0.2,    # total_nitrogen\n",
    "    232.52, # potassium\n",
    "    45.23,  # p2o5\n",
    "    0.64,   # boron\n",
    "    2.31,   # zinc\n",
    "    45.16,  # sand\n",
    "    22.42,  # clay\n",
    "    31.5,   # slit\n",
    "    2.0,    # parentsoil (encoded, e.g., 2)\n",
    "    0.0,    # crop (encoded)\n",
    "    1.0     # variety (encoded)\n",
    "]\n",
    "\n",
    "# 2 Run inference\n",
    "pred_mean, pred_percent = predict_fertilizer_with_percent_uncertainty(manual_input, mc_runs=50)\n",
    "\n",
    "# 3 Display results\n",
    "for f, m, p in zip(fert_cols, pred_mean, pred_percent):\n",
    "    print(f\"{f}: {m:.2f} units  {p:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7441efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
